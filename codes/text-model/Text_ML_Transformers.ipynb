{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (3.19.6)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: emoji in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stanza) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->stanza) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->stanza) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->stanza) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->stanza) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from textblob) (3.6.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.1->textblob) (2021.11.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk>=3.1->textblob) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from xgboost) (1.9.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n",
    "!pip install textblob\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import stanza\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import emoji\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from textblob import TextBlob \n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from tqdm import tqdm\n",
    "# ---- Call tqdm to see progress bar with pandas\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(object):\n",
    "    '''\n",
    "    Class to preprocess text\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        #print(\"Welcome in the preprocessing\")\n",
    "        pass\n",
    "    \n",
    "    @classmethod    \n",
    "    def detect_lang_google(self, x):\n",
    "        '''\n",
    "        Function to detect the language of the string\n",
    "        @param x: (str) sentences of text to detect language\n",
    "        @return: (str or nan) language of the sentence\n",
    "        '''\n",
    "        translate = Translator()\n",
    "        try:\n",
    "            return translate.detect(x).lang\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_numbers(self, text):\n",
    "        '''\n",
    "        Function to remove number in text.\n",
    "        @param text: (str) sentence\n",
    "        @return: (str) clean text\n",
    "        '''\n",
    "        text = ''.join([i for i in text if not i.isdigit()])         \n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_URL(self, text):\n",
    "        '''\n",
    "        Function to remove url from text.\n",
    "        @param text: (str) sentence\n",
    "        @return: (str) clean text\n",
    "        \n",
    "        '''\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'',text)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_html(self, text):\n",
    "        '''\n",
    "        Function regex to clean text from html balises.\n",
    "        @param text: (str) sentence \n",
    "        @return: (str) clean text \n",
    "        '''\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'',text)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_emoji(self, text):\n",
    "        '''\n",
    "        Function to remove emojis, symbols and pictograms etc from text\n",
    "        @param text: (str) sentences \n",
    "        @return: (str) clean text \n",
    "        '''\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess(self, text):\n",
    "        '''\n",
    "        Function to remove special characters\n",
    "        @param text: (pandas dataframe) text\n",
    "        @return: (pandas dataframe) clean text \n",
    "        '''\n",
    "        text = text.replace(\"(<br/>)\", \"\")\n",
    "        text = text.replace('(<a).*(>).*(</a>)', '')\n",
    "        text = text.replace('(&amp)', '')\n",
    "        text = text.replace('(&gt)', '')\n",
    "        text = text.replace('(&lt)', '')\n",
    "        text = text.replace('(\\xa0)', ' ')  \n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"\\x92\", \"'\")\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_char_specific(self, text):\n",
    "        '''\n",
    "        Function to remove specific characters\n",
    "        @param text: (str) text\n",
    "        @return: (str) text without specific characters\n",
    "        '''\n",
    "        table = '!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~•'\n",
    "        table = str.maketrans(' ', ' ', table)\n",
    "        words = text.split()\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        return ' '.join(stripped)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_upper_case(self, text):\n",
    "        '''\n",
    "        Function to transform upper string in title words\n",
    "        @param text: (str) text \n",
    "        @return: (str) text without upper words \n",
    "        '''\n",
    "        words = text.split()\n",
    "        stripped = [w.title() if w.isupper() else w for w in words]\n",
    "        return \" \".join(stripped)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_stop_words(self, x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_words(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent unigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer().fit(corpus)             # bag of words\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0)  \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_words_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent unigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(stop_words = \"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_bigram(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent bigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus) \n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_bigram_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent bigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_trigram(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "\n",
    "    @classmethod\n",
    "    def get_top_n_trigram_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(3, 3), stop_words=\"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_5grams_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(5, 5), stop_words=\"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7V9xGKRip1F1"
   },
   "outputs": [],
   "source": [
    "class Metrics(object):\n",
    "    '''\n",
    "    Class containing functions to plot the different metric curves (Precision-Recall, ROC AUC etc...)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialisation of the class'''\n",
    "        \n",
    "        \n",
    "\n",
    "    @classmethod \n",
    "    def roc_auc_curve(self, model, x, y, labels, gb=False):\n",
    "        '''\n",
    "        Function to plot the ROC AUC curves for binary or multiclass classification. \n",
    "        Correct for standard machine learning models and Neural Networks. \n",
    "        @param model: (model) classification model\n",
    "        @param x: (list) validation sample\n",
    "        @param y: (list int) validation sample label\n",
    "        @param gb: (bool) inform if the model is an ensemble model \n",
    "        '''\n",
    "        # generate a no skill prediction (majority class)\n",
    "        ns_probs = [0 for _ in range(len(y.reshape(-1, 1)))]\n",
    "        # predict probabilities\n",
    "        if gb: # test if ensemble model \n",
    "            lr_probs = model.predict_proba(x)\n",
    "        else:    \n",
    "            lr_probs = model.predict(x)\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        if len(labels)==2: # binary classification \n",
    "            if gb:\n",
    "                # compute area under the roc curve\n",
    "                lr_auc = roc_auc_score(y, lr_probs[:,1], average=\"weighted\")\n",
    "            else:\n",
    "                lr_auc = roc_auc_score(y, lr_probs, average=\"weighted\")\n",
    "            # Compute no skill roc curve \n",
    "            ns_fpr, ns_tpr, _ = roc_curve(y, ns_probs)\n",
    "            # plot the curve no skill\n",
    "            plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "            if gb: # test if the model is an ensemble model\n",
    "                # compute ROC curve\n",
    "                lr_fpr, lr_tpr, _ = roc_curve(y, lr_probs[:,1])\n",
    "            else:\n",
    "                # compute ROC curve\n",
    "                lr_fpr, lr_tpr, _ = roc_curve(y, lr_probs)\n",
    "\n",
    "            # plot the roc curve for the model\n",
    "            plt.plot(lr_fpr, lr_tpr, label=f'Class (area {round(lr_auc,3)})')\n",
    "                # axis labels\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "                # show the grid\n",
    "            plt.grid(True)\n",
    "                # show the legend\n",
    "            plt.legend()\n",
    "        else: # multilabel classification \n",
    "            dummy_y = np_utils.to_categorical(y)\n",
    "            #ns_auc = roc_auc_score(valid_y, dummy_ns, average=\"weighted\", multi_class=\"ovr\")\n",
    "\n",
    "            lr_auc_multi = []\n",
    "            for i in enumerate(labels):\n",
    "                lr_auc_multi.append(round(roc_auc_score(dummy_y[:,i[0]], lr_probs[:,i[0]], average=\"weighted\"),3))\n",
    "                print(f\"ROC AUC class {i[1]}: {lr_auc_multi[-1]}\")\n",
    "            lr_auc = roc_auc_score(dummy_y, lr_probs, average=\"weighted\", multi_class=\"ovr\" )\n",
    "\n",
    "            ns_fpr, ns_tpr = [i/10 for i in list(range(0, 11, 1))], [i/10 for i in list(range(0, 11, 1))] #f(range(0, 0.1, 1))\n",
    "            plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "            for i in range(lr_probs.shape[1]):\n",
    "                lr_fpr, lr_tpr, _ = roc_curve(dummy_y[:,i], lr_probs[:,i])\n",
    "                # plot the roc curve for the model\n",
    "                plt.plot(lr_fpr, lr_tpr, label=f'Class {labels[i]} (area {lr_auc_multi[i]})')\n",
    "                # axis labels\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                # show the grid\n",
    "                plt.grid(True)\n",
    "                # show the legend\n",
    "                plt.legend()\n",
    "                \n",
    "        print('\\nROC AUC=%.3f \\n' % (lr_auc))\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod \n",
    "    def confusion_matrix(self, model, y, x, labels):\n",
    "        '''\n",
    "        Compute the confusion matrix for binary or multiclass classification. \n",
    "        Correct for standard machine learning models and Neural Networks. \n",
    "        @param model: (model) classification model\n",
    "        @param x: (list) validation sample\n",
    "        @param y: (list int) validation sample label\n",
    "        \n",
    "        '''\n",
    "        if len(labels)==2: # binary confusion matrix\n",
    "            confu_matrix = pd.DataFrame(confusion_matrix(y, (model.predict(x) > 0.5).astype(int)), \\\n",
    "                 columns=['Predicted Negative', \"Predicted Positive\"], index=['Actual Negative', 'Actual Positive'])\n",
    "            print(confu_matrix)\n",
    "            return confu_matrix\n",
    "        else:\n",
    "            # multiclass confusion matrix\n",
    "            dummy_y = np_utils.to_categorical(y)\n",
    "            mcm = multilabel_confusion_matrix(dummy_y, np_utils.to_categorical(model.predict(x).argmax(-1)))\n",
    "            df_mcm = pd.DataFrame()\n",
    "            for i in zip(mcm, labels): # compute confusion matrix for each class \n",
    "                mcm = pd.DataFrame(data=i[0], columns=['Predicted Negative', \"Predicted Positive\"], index=['Actual Negative', 'Actual Positive'])\n",
    "                df_mcm = df_mcm.append(mcm)\n",
    "                print(\"\\nConfusion matrix for classe: %s \\n\" %(i[1]))\n",
    "                print(mcm)\n",
    "                print(\"\\n\")\n",
    "            return df_mcm\n",
    "        \n",
    "    @classmethod \n",
    "    def precision_recall_curve(self, model, x, y, labels, gb=False):\n",
    "        '''\n",
    "        Function to plot the recall precision curves for binary or multiclass classification. \n",
    "        Correct for standard machine learning models and Neural Networks. \n",
    "        @param model: (model) classification model\n",
    "        @param x: (list) validation sample\n",
    "        @param y: (list int) validation sample label\n",
    "        @param gb: (bool) inform if the model is an ensemble model \n",
    "        '''\n",
    "        # predict probabilities\n",
    "        if gb: # test if the model is an ensemble model \n",
    "            lr_probs = model.predict_proba(x)\n",
    "        else:\n",
    "            lr_probs = model.predict(x)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        plt.figure(figsize=(10,8))\n",
    "\n",
    "        if len(labels)==2: # binary classification \n",
    "            if gb:\n",
    "                precision, recall, thresholds = precision_recall_curve(y, lr_probs[:,1]) # compute precision recall curve \n",
    "                lr_f1 = f1_score(y,(lr_probs[:,1]>0.5).astype(int))\n",
    "            else:\n",
    "                precision, recall, thresholds = precision_recall_curve(y, lr_probs)\n",
    "                lr_f1 = f1_score(y,(lr_probs>0.5).astype(int))\n",
    "            \n",
    "            # calculate precision-recall AUC\n",
    "            lr_auc = auc(recall, precision)\n",
    "            # summarize scores\n",
    "            print('Model: f1-score=%.3f AUC=%.3f' % (lr_f1, lr_auc)) # print f1-score and auc \n",
    "            plt.plot(recall, precision, marker='.', label='Model')\n",
    "            no_skill = len(y[y==1]) / len(y)\n",
    "            plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "        else:\n",
    "            dummy_y = np_utils.to_categorical(y)\n",
    "            dummy_lr = np_utils.to_categorical(lr_probs.argmax(-1))\n",
    "            for i in enumerate(labels):\n",
    "                precision, recall, thresholds = precision_recall_curve(dummy_y[:,i[0]], lr_probs[:,i[0]])\n",
    "                # calculate precision-recall AUC\n",
    "                lr_f1 = f1_score(dummy_y[:,i[0]], dummy_lr[:,i[0]]) \n",
    "                lr_auc = auc(recall, precision)\n",
    "                # summarize scores\n",
    "                print('Model class: %s --> f1-score=%.3f AUC=%.3f' % (i[1], lr_f1, lr_auc))\n",
    "                plt.plot(recall, precision, label='Class %s' %(i[1]))\n",
    "            no_skill = len(y[y>=1]) / len(y)\n",
    "            plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "        # plot the precision-recall curves\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # axis labels\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the grid\n",
    "        plt.grid(True)\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod \n",
    "    def plot_eval_xgb(self, model, labels):\n",
    "        '''\n",
    "        Function to plot the evaluation curves for xgboost models \n",
    "        @param model: (model) xgboost model\n",
    "        @param labels: (list) list ocntaining the labels in string \n",
    "        '''\n",
    "        # retrieve performance metrics\n",
    "        results = model.evals_result()\n",
    "        if len(labels)>2: # multiclass \n",
    "            log_ = \"mlogloss\"\n",
    "            error_= \"merror\"\n",
    "        else: # binary classifiation\n",
    "            log_ = \"logloss\"\n",
    "            error_= \"error\"\n",
    "\n",
    "        # create axis x with the number of epochs\n",
    "        epochs = len(results['validation_0'][error_])\n",
    "        x_axis = range(0, epochs)\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.subplot(221)\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.plot(x_axis, results['validation_0'][log_], label='Train')\n",
    "        plt.plot(x_axis, results['validation_1'][log_], label='Test')\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title('XGBoost Log Loss')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True)\n",
    "\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(222)\n",
    "        plt.plot(x_axis, results['validation_0'][error_], label='Train')\n",
    "        plt.plot(x_axis, results['validation_1'][error_], label='Test')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Classification Error')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title('XGBoost Classification Error')\n",
    "        plt.legend( loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    @classmethod \n",
    "    def plot_confusion_matrix(self, cm, classes, normalized=True, cmap='bone'):\n",
    "        '''\n",
    "        Function to generate an heatmap of the confusion matrix\n",
    "        @param cm: (matrix) confusion matrix\n",
    "        @param classes: (list) list containing labels of the classes\n",
    "        @param normalised: (bool) determined if the confusion matrix need to be normalized\n",
    "        @param cmap: (str) color for the confusion matrix\n",
    "        '''\n",
    "        plt.figure(figsize=[10, 8])\n",
    "        norm_cm = cm\n",
    "        if normalized:\n",
    "            norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n",
    "            #plt.savefig('confusion-matrix.png')\n",
    "\n",
    "    @classmethod\n",
    "    def plot_history(self, history):\n",
    "        '''\n",
    "        Function to plot the learning curves of a neural network\n",
    "        @param history: metrics of a neural network\n",
    "        '''\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.subplot(221)\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.grid(True)\n",
    "\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(222)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "            \n",
    "            \n",
    "    @classmethod\n",
    "    def metrics_deep_learning(self, model, history, x, y, labels):\n",
    "        '''\n",
    "        Function to plot the different metrics for the deep learning algorithms.\n",
    "        @param model: (tensorflow.python.keras.engine.sequential.Sequential) deep learning model\n",
    "        @param history: (tensorflow.python.keras.callbacks.History) history of the training model\n",
    "        @param x: (numpy.ndarray) x data\n",
    "        @param y: (numpy.ndarray) target data\n",
    "        @param labels: (list) list containing the labels in str  \n",
    "        '''\n",
    "        self.plot_history(history)\n",
    "        if len(labels)==2:\n",
    "\n",
    "            print(classification_report(y, (model.predict(x) > 0.5).astype(int), target_names=labels))\n",
    "            print(f\"\\nThe balanced accuracy is : {round(100*balanced_accuracy_score(y, (model.predict(x)>0.5).astype(int)),2)}%\\n\")\n",
    "            print(f\"\\nThe Zero-one Loss is : {round(100*zero_one_loss(y, (model.predict(x)>0.5).astype(int)),2)}%\\n\")\n",
    "            print(f\"\\nExplained variance score: {round(explained_variance_score(y, (model.predict(x)>0.5).astype(int)),3)}\\n\" )\n",
    "            self.roc_auc_curve(model, x, y, labels)\n",
    "            self.precision_recall_curve(model, x, y, labels)\n",
    "\n",
    "            print(f\"\\nCohen's kappa: {round(100*cohen_kappa_score(y, (model.predict(x) > 0.5).astype(int) ),2)}% \\n\") \n",
    "            #matrices = self.confusion_matrix(model, y, x, labels)\n",
    "            cm = confusion_matrix(y, (model.predict(x) > 0.5).astype(int))\n",
    "\n",
    "            print(\"\\nConfusion Matrix\\n\")\n",
    "            self.plot_confusion_matrix(cm, labels)\n",
    "        else:\n",
    "\n",
    "            print(f\"\\nThe balanced accuracy is : {round(100*balanced_accuracy_score(y, model.predict(x).argmax(axis=-1)),2)}%\\n\")\n",
    "            print(f\"\\nThe Zero-one Loss is : {round(100*zero_one_loss(y, model.predict(x).argmax(axis=-1)),2)}%\\n\")\n",
    "            print(f\"\\nExplained variance score: {round(explained_variance_score(y, model.predict(x).argmax(axis=-1)),3)}\\n\" ) \n",
    "            self.roc_auc_curve(model, x, y, labels)\n",
    "            self.precision_recall_curve(model, x, y, labels)\n",
    "\n",
    "            print(f\"\\nCohen's kappa: {round(100*cohen_kappa_score(y, model.predict(x).argmax(axis=-1) ),2)}%\\n\")\n",
    "            #matrices = self.confusion_matrix(model, y, x, labels)\n",
    "            cm = confusion_matrix(y, model.predict(x).argmax(axis=-1))\n",
    "\n",
    "            print(classification_report(y, model.predict(x).argmax(axis=-1), target_names=labels))\n",
    "\n",
    "            print(\"\\nConfusion Matrix\\n\")\n",
    "            self.plot_confusion_matrix(cm, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(clf, X, y, x_test, y_test):\n",
    "    '''\n",
    "    Function to compute a classifier model\n",
    "    @param clf: (model) classifier model\n",
    "    @param X: (matrix) training x data\n",
    "    @param y: (list) train labels - ground truth\n",
    "    @param x_test: (matrix) matrix of test x data\n",
    "    @param y_test: (list) list of test labels\n",
    "    @return model_NB: (model) train model\n",
    "    @return pred: (list) list of predicted labels\n",
    "    @return end: (float) fit time of the model\n",
    "    @return model.score(): (float) accuracy of the model\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(X, y)\n",
    "    end = time.time() - start\n",
    "    pred = clf.predict(x_test)\n",
    "    \n",
    "    #accuracy = accuracy_score(y_test, pred)\n",
    "    \n",
    "    return clf, pred, end, clf.score(x_test, y_test)#, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "def metrics_ML(clf, X, Y, X_test, Y_test, labels, gb=False):\n",
    "    '''\n",
    "    Function to compute a classifier model\n",
    "    @param X: (matrix) training x data\n",
    "    @param y: (list) train labels - ground truth\n",
    "    @param x_test: (matrix) matrix of test x data\n",
    "    @param y_test: (list) list of test labels\n",
    "    @return model_NB: (model) train model\n",
    "    @return pred: (list) list of predicted labels\n",
    "    @return end: (float) fit time of the model\n",
    "    @return model.score(): (float) accuracy of the model\n",
    "    '''\n",
    "    model_, pred, time_train, score_ = classifier_model(clf, X, Y, X_test, Y_test)\n",
    "    print(\"Execution time : %.3f s\" %(time_train))\n",
    "    print(f\"Score : {round(100*score_,2)} %\" )\n",
    "    print(\"\\nClassification Report\\n\")\n",
    "    print(classification_report(Y_test, pred, target_names=labels))\n",
    "    cm = confusion_matrix(Y_test, pred)\n",
    "    print(\"\\nConfusion Matrix\\n\")\n",
    "    Metric.plot_confusion_matrix(cm, labels)\n",
    "    '''\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    Metric.precision_recall_curve(model_,  X_test, Y_test, labels, gb=gb)\n",
    "    print(\"\\n\")\n",
    "    Metric.roc_auc_curve(model_,  X_test, Y_test, labels, gb=gb)\n",
    "    print(f\"\\n\\nCohen's kappa: {round(100*cohen_kappa_score(Y_test,  pred),2)}%\\n\\n\")\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = PreProcessing()\n",
    "Metric = Metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path, n, label):\n",
    "  x = []\n",
    "  for i in range(n):\n",
    "    T = \"\"\n",
    "    #print(path)\n",
    "    with open(str(path + str(i+1)+'.txt'),encoding=\"utf8\") as f:\n",
    "      lines = f.readlines()\n",
    "      for e in lines:\n",
    "        T = T + e\n",
    "    x.append(T)\n",
    "  L = np.zeros(len(x))\n",
    "  if(label == 1):\n",
    "    L = np.ones(len(x))\n",
    "  return x, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ['D:/Co/Sem7/Prj/MiniProj/Abusive/Abusive-',\n",
    "     'D:/Co/Sem7/Prj/MiniProj/NonAbusive/Non-Abusive-']\n",
    "N = [40, 47]\n",
    "L = [0, 1]\n",
    "Text_nab, Lbl_nab = load_text(p[1] ,N[0], L[0])\n",
    "Text_ab, Lbl_ab = load_text(p[0], N[1], L[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['அஞ்சு வருஷம் உங்க பவர் காமிச்சீங்களே என்ன ஆட்டம் போட்டீங்க எடிமகே  சுண்ணிக்கே எங்க தளபதி அத தடுக்கத்தாண்டா வராரு அவுர மட்டும் அழிக்கணும்னு நினைச்ச முடியவே முடியாது நீ அதுக்கு தகுதியே கிடையாது எடிமகே ஒ சுண்ணிக்கே ஒ நீ அதுக்கு தகுதியே கிடையாது என் தளபதி தான் இந்த நாட்டையே இந்த நாட்டையே நல்லவிதமா மாத்துறது எங்க தளபதி மட்டும்தான் நீ அந்த அஞ்சு வருஷம் என்ன பண்ணமுடியும் இந்த மக்களுக்காக இந்த மக்கள் என்ன பண்ணப்போறாய்ங்க அந்த தைரியத்தில தானே நீக்க இப்படி ஆடுறிங்க இந்த சர்க்கார் படம் வந்ததுல தாண்ட தெரியுது உங்க ஊழல் புண்ட என்னடா புண்டைகளா சிரிப்புத்தாண்டா வருது உங்கள பாத்த உங்களப்பத்தி பேசுனாலே சிரிப்பு புண்ட தாண்ட வருது கோவா புண்டைய வருது ஜோக்கர் புண்டடா நீங்கல்லாம் இப்படி பேசுறாங்களே அவமான புண்டையா இல்லையா     ',\n",
       " 'நரேந்திர மோடியா நீதிபதியா வந்து நரேந்திர மோடிக்கு மூனு ஆயுள் தண்டனை ரெண்டு தூக்கு தண்டனை போடுறமாரி நீதிபதியா வந்து நடி குஜராத் கலவரத்தை விசாரிக்கும் அதிகாரியவ நீதிதுறை அதிகாரியவ நடி சுப்ரீம் கோர்ட் ஜுட்ஜ் ஆய்க்கோ அதுல வேணும்ன்னா நானே வக்கீலா நடிக்குறேன் பரவலா என்னை அணைபோட்டு தூக்கிட்டாலும் பரவல நானே வக்கீலா நடிக்குறேன் அதுல நரேந்திர மோடிக்கு நீ  திருப்பு சொல்ற நான் வாதாடுரேன் நீ தீர்ப்பு சொல்ல நரேந்திர மோடிக்கு மூனு ஆயுள் தண்டனை ரெண்டு தூக்கு தண்டனை நீ தீர்ப்பு சொல்ற நான் உன்னோட கட்சில நாளைக்கே சேர்ரென் வட விஜய் உன் அமைப்புல நாளைக்கே சேர்ரென் விஜய் நாசமபோற நாய் கட்சில சேர்ரென் வட நீ வரமாட்ட நல்ல துட்டையும் வாங்கிட்டு நடிகைய நல்ல தடவிக்கிட்டு எசி ரூம்ல படுத்துக்கிட்டு வாய்யா அருக்குற மாறி படம் ஏன்டா பண்டார பயலே பண்டார பயலே நாற்றம் பிடிச்ச நடிகன் விபச்சாரத்தை பெருக்கின்ற அயோக்கியன் பச்சிளம் பெண்களை சின்ன சின்ன குழந்தைகள் பசங்கள் பிள்ளை வயசுலல்லாம் காதல் காதல் காதல் இல்தகசய்ய உங்க அக்காவ போட்டு செய்ட    ',\n",
       " 'ஒன்னும் தெரியாத மண்ணு கூதியே எடப்பாடி பழனி சம்மி முதலமைச்சர இருக்கீங்களே எங்க தளபதிக்கு எல்லாமே தெரியும் அரசியல பத்தி அவர முதலமைச்சர போட்ட உங்களுக்கு என்ன நீக்க போட்ட ஓட்டவிட எங்க தளபதிக்கு எவ்வளவு ஓட்டு இப்போ இளைஞர்கள் எவ்ளோ பேரு தளபதிக்காக உயிர குடுக்க தயார இருக்காங்க உங்களுக்கு இருக்காங்கள எடப்பாடி சுண்ணிக்கு இருக்காடா எங்க தளபதி தங்க தளபதிக்கு இருக்கு ஓத்தலாக தண்ணிய ஒளிச்சிங்க தண்ணி பிரச்சனை வந்துச்சு அப்போ எங்க ஊம்ப போனிங்க விவசாய்த்த ஒளிச்சிங்க அப்போ எங்க ஊம்ப போனிங்க அதெல்லாம் கேக்க மாட்டீங்க சாதாரணம் படம் வந்ததுக்கு அப்புறம் குத்துதே கொடையுதேன எப்படிங்க எங்க தளபதி சொன்னது தான் கரெக்ட் நீக்கலாம் வேஸ்ட்த்தான் ஓகேவ நீக்க தான் முதல் திருடன் ஆட்சில இருக்கவைக்கலாம் முதல் திருடன்   ',\n",
       " 'அதே மாதிரி இந்த பொண்ணுங்களம் வந்து கலை நான் உன்ன லவ் பண்றேன்னு சொல்லிட்டு இருந்திங்கனா டென்ஷன் ஆயிடுவேன் ஒரு பொண்ணு கமெண்ட் பன்னிருக்கு சுஜினு ஒரு பொண்ணு தலைவரே நான் உங்கள லவ் பண்றேன் என் புருஷன டைவோர்ஸ் பண்ணிட்டு கல்யாணம் பண்ணிக்குறேன் ஐட்டம் மூதேவி ஐட்டம் உண்ணாக்கு கல்யாணம் ஆயிடுச்சு முதல அத சொல்ல வேண்டியதுதானே தலைவரே நான் உங்கள லவ் பண்றேன்னு சொன்ன உடனே நானும் ஆசைய அப்படிங்கள இன்ஸ்டவுள உடனே மெசஜ் பண்ணிக்கிட்டு இருக்கும் பொது புருஷன டைவோர்ஸ் பண்ணிட்டு கல்யாணம் பண்ணிக்குறேன் குழந்தைய யாரு வழக்குறது நீக்கலாம் பொண்ணா ச்சீ இந்த உலகத்துல யாரையும் நம்பாதிங்க நம்மளோடைய கைகள்ல மட்டும் நம்புங்க யாரையும் நம்பாதிங்க ப்ரண்ட்ஸ்           ',\n",
       " 'இதுல வந்து என்னப்பதி தப்ப பேசுறது இதெல்லாம் வெச்சிகிட்டீங்கனா நான் மதப்பையன் மாதிரி கிடையாது நான் சொல்றதையும் செய்வேன் சொல்லாததையும் செய்வேன் என்கிட்ட எல்லா திறமையம் இருக்கு இதே மாதிரி இந்த ஷீலா கேளடு மூதெய்வி அவ மட்டும் என்ன கைல கிடைச்ச மயிறு அறுத்துருவேன் பாத்துக்க நீ ஒரு பொண்ண உண்ணலாம் எப்படி தான் பெத்தாங்களோ தெரியல கேவலமான பிறவி  இந்த பிறவி மாதிரி நீங்களும் அய்டாதீங்க லவ் பண்ணி எமத்திட ஷீலா அக்கா என்ன என்னக்கு அவங்கள சுத்தமா பிடிக்காது ஷீலா அக்காவ அவுங்க ஒரு கேவலமான கேரக்டர் அதே மாதிரி திவ்யாவும் ஐயோ இன்ஸ்டால யூடூப்ல யாரையும் பாதி பேசமுடியாத நீங்களே சொல்லுங்க யாரு நல்லவங்க சொல்லுங்க நீங்க',\n",
       " 'இன்ஸ்டால யூடூப்ல செலிபிரிட்டி யாரு நல்லவங்க நீங்களே சொல்லுங்க சாதன ஆண்ட்டியா சூரிய ஆண்ட்டியா இல்ல ஷீலா அக்காவா இல்ல திவ்யாவா நீக்க கமெண்ட் பண்ணுங்க பாப்போம் யாருமே சொல்ல முடியாது இங்க ஐட்டம் அங்க ஐட்டம் சீ எங்க பாத்தாலும் ஐட்டம் ஆஹ் அலஞ்சுகிட்டு ஜி பி முத்து அந்த ஆளு ஒரு ஆம்பள ஐட்டம் அரிச்சலவள என்னக்கு இதெல்லாம் சுத்தமா பிடிக்காது திவ்யா என்னடானா எல்லாரையும் கட்டிபிடிச்சுக்கிட்டு ரேப் பண்ணிக்கிட்டு இதெல்லாம் ஒரு பொல்லப்பா ஒரு பொண்ணு தான நீ உனக்கு கற்புனு ஒன்னு இருக்காது கடவுள் படைக்கல உன்னக்கு கற்புன்றது என் இப்படிலாம் பண்ற எல்லாத்தையும் கடவுள் பாத்துட்டு தான் இருக்காரு தயவுசெஞ்சு திருந்தி வாழுங்க எல்லாரும்',\n",
       " 'மச்சி பிரசன்னா எப்டி மச்சி, அவன் ஆ தூ, மயிரு குத்தி அவன் மயிரு குத்தி, அவன் ஆ நம்ப முடியது. அவன் ஆ பத்தி பேசுனா வாய் ல அமிலம் ஊத்தி கழுவானும். வாயில இல்ல மச்சி சுத்துல. நா திடுற அவன் ஆ, அவன் ஒரு லூசு குத்தி, தைக்கிள்ளி, ஃப்ளர்ட் நா அவன் ஃப்ளர்ட். அவளோ ஃப்ளர்ட். பொண்ணு ங்க சுத்துல அவன் கேடி. பொண்ணு பொண்ணு, கண்றாவி குதி அவன். கடைசி ல அவன் கேன்சர் ஓ எச்ஐவி ஓ தான் வந்து சாவனும். எச்ஐவி தான் வரணும். அவன் காதலி யாரோ, காதலி லா நம்பதிங்கோ, நா சொல்றேன், அவன் ஆ நம்பி போன சத்தியம் தோல்வி தான். சத்தியம் ஆ சுத்து கிழிச்சிடு விடுவான்.',\n",
       " 'கோமா உண்ட ஓத்தா புண்டை கண்டறி ஒதால ல கு கோமா உண்ட நா  உன்கிட்ட நீ எவன் கிட்ட ந சொல்லு போ, போ பச்சகாரா சுண்ட, பச்சகாரா கோமா, ஆ மூடுற ஒதாலகா ஏகா அட மூடுறா ஓத்தா புண்டை அட வாடா ',\n",
       " 'வீட்டுல அழகான புது மெத்தை இருந்தும், நெறய பேருக்கு  ஓட்டை விழுந்தா பாய்ல்  ரெஸ்ட் எடுக்க  தான் புடிச்சி இருக்கு. உங்கலுக்கு டைட்டில் ல இருந்தே புரிஞ்சி இருக்கும் நான் என்ன சொல்ல வரேன் னு. புரியாத மக்கு சாம்புரானிங்க தயவு செஞ்சி இந்த வீடியோ பாக்க வேண்டாம் . பொண்ணுங்க வயசு ஆக ஆக நல்ல கும்முனு இருப்பாங்க, அதுனாலயோ என்னமோ பசங்க வண்டியா அந்த பக்கம் ஆ திருப்பிடுறாங்க. கும்முனு இருந்தா மட்டும் போதும் ஆ? வயசு ஆக ஆக உள்ள இருக்க சுபைர் பார்ட்ஸ்  எல்லாம் ரிப்பேர் ஆயிடும். அது தெரியாம பசங்க ஓட்ட வண்டிய ஹை ஸ்பீட் ல ஓட்டணும் னு நெனைச்சா நேராக ஆ விபத்து தா',\n",
       " 'புருஷன புடிக்கல னு தான் உங்கள உறவு வெக்க சம்மந்தம் தெரிவிக்குறாங்க. நாளைக்கு அந்த ஆள போட்டு தள்ளனும் ந உங்கள தான் யூஸ் பண்ணுவாங்க. உங்கள வீரா தீரா சூரன் னு சொல்லி நல்ல உசுப்பேத்துவாங்க. நீங்களும் என்ன நடக்குது னு புரியாம பெரிய ரிஸ்க் ல இன்வோல்வ் ஆய்டுவீங்க. சில அறிவாளிங்க மேட்டர் மட்டும் பண்ணிட்டு அப்படியே அப்ஸ்காண்ட் அய்டலாம் னு பிளான் பண்ணுவாங்க. ஆனா அந்த ஆண்ட்டிகளோட பிளான் எ வேற. உங்களோட முழு ஜாதகமும் அவங்களுக்கு தெரியும், நீங்க எங்கேயும் ஓட முடியாது.',\n",
       " '... கிட்ட பசங்க கவனமா இருக்கனும். ஒரு கட்டத்துல டீசென்ட் ஆஹ் விலகி போகாம, பெரிய சண்டை ஆஹ் போடு கெளம்பனும் னு நெனச்சீங்கனா. இந்த எஸ்பிஎரிஎன்ஸ் ஆனா ஆண்ட்டிங்க உங்க வாழ்க்கை ஆஹ் வே காலி பன்னிடுவாங்க. நீங்க வேல பண்ற எடத்துல கண்ட படி மெயில் அனுப்புறது தொடங்கி, உங்க பேமிலி ரெளடிவ்ஸ் கு உங்கள போட்டு குடுக்குற வரைக்கும் சிறப்பா செஞ்சிடுவாங்க. சோ ஒரு முறை கு நூறு முறை நல்லா யோசிச்சிக்கோங்க. ஆண்ட்டி ஆஹ் ரூட் விடுறது சேரி ஆஹ் இல்லையா னு. ஒகே பிரிஎண்ட்ஸ் அவளோ தான் இந்த வீடியோ மீண்டும் ஒரு சுவாரஸ்யமான வீடியோ ஓட அடுத்த செக்மென்ட் ல சந்திக்கலாம்.  ',\n",
       " 'ஸ்டாப் கமெண்ட்டிங் ஒன் மை லைப். ஐபி யு டூ லிட்டரல்லி. ஓத்தா தேவிடியா கு பொறந்தவனே எவளோ மரியாதையா சொல்லிட்டு இருக்கேன். தமிழ் ல தான ஓக்கணும் வா ஓக்குறேன். உனக்கு என்ன பிரச்சனை. இல்ல எனக்கு புரில ஓத்தா, நானும் எவலோவோ பாத்துட்டு இருக்கேன். எவலோவோ நேரத்துக்கு நீ பண்ற கமெண்ட் கு ல அமைதியா இருக்கேன் ஓத்தா நீ எங்கயோ மூளை ல உட்க்காந்துட்டு ஓத்தா சும்மா நக்கல் புண்டை பண்ணிட்டு இருக்காதா ந. நா பேசுற வார்த்தை ல மட்டும் பேசுறேன் னு நெனைச்சுகாதா, ஓத்தா நேர்ல வந்தன சூத்து கீது ல கிழிச்சு அனுப்பிச்சிடுவேன். என்ன நினைச்சிட்டு இருக்க ஹான்? என்ன எல்லா பொண்ணு இந்த மாறி நீ பண்ற எல்லா கமெண்ட் கு நீ பேசுறதுக்கு எல்லாம் ஓத்தா அமைதியா இருப்பேன் னு நினைச்சிட்டு இருக்கியா? நா அப்படி கெடையாது ஓபிவியஸ்ஆஹ். நோட் ஒன்லி இந்த போன் ஓத்தா, மெசேஜ் மட்டும் இல்ல, லைவ் ல மட்டும் இல்ல, நேர்லயும் வந்து அசிங்க அசிங்க மா கேப்பேன். அசிங்க அசிங்கமா கிழிச்சிடுவேன்.ரோடு நாலு பேரு பாக்குற மாறி அசிங்க படுத்திடுவேன். என்ன யாரையும் எங்க இருக்கீங்க னு கண்டு புடிக்க முடியாது ன்ற அதுப்புல டெஸ்ட் பண்ணிட்டு இருக்கீங்களா எனக்கு. ',\n",
       " 'பாக் யு, யு அஷோல். வா வா எங்க இருக்க சொல்லு வரேன். அப்படியே ஓத்தா காண்டொம் ஆஹ் போட்டு தண்ணி புடிச்சி திருப்பி உன் பூல் ல உடுக்கோ. என்ன எப்போ பாரு சும்மா கமெண்ட் பண்ணா. தேரே ஐஸ் எ லிமிட் ஒகே. ஸ்டாப் இட். ஒகே இ அம் ரேஅல்லி சாரி. சோ மச் ஒப்பி பெஒப்லே தேர் லைக் வதோ ரெஸ்பெக்ட்ஸ் மீ எ லோட்.\\n',\n",
       " 'என் பக்கத்துலயே இருக்க, ஓத்தா என் பக்கத்துல இருந்த நீ இந்நேரத்துக்கு உயிரோட இருக்க மாட்ட டா பாடு. உன் பூலு இருக்கு ல அப்படியே ரெண்டா கிழிச்சி இருப்பேன், கிழிச்சி ஓத்தா உன் சூத்து கீது எல்லாம் ஓத்தா உன்னையே ரெண்டா கிழிச்சி இருப்பேன். என்ன சும்மா இங்கிலிஷ் ல, நா படிக்கல உண்மை உண்மை ஆஹ் சொல்லனும் ந பத்தாவது தான் படிச்சி இருக்கேன். நா பேசுறது எல்லாம் நான் ஆஹ் காதுகுடத்து தான். ஒகே? சோ ஸ்டாப். நா வந்து திருப்பி திருப்பி திருப்பி திருப்பி திருப்பி உங்க கிட்ட ல ஓத்தா காத்திடு இருக்கானு ன்ற அவசிய புண்டை எனக்கு கிடையாது. ஐபி யு டூ தட் அகைன் ஐ வில் செறிவுசலை ஐ வில் புட் மோர் போஸ்டல் அண்ட் மோர் ளைவ்ஸ் ஒன்லி போர் யு லைக் யு அசோலஸ். லுக் எவேர்யோனே ஐஸ் பாஸிங் சோ மச் ப்ரோப்லேம்ஸ் இன் தெயர் லைப். எவெரிஒன் ஹாஸ் தெயர் ஓவ்ன் ஷிட்ஸ் டு கிலீர். ஸ்டாப் புள்ளயிங் தேம். ஸ்டாப் டூ திஸ் அகைன் அண்ட் அகைன். பை செல்லம் ஆஹ் ஓத்தா, போடா பாடு, வண்டா பை சொல்றதுக்கு. வா குடு வா ஓத்தா இந்த ஆணி செருப்பு வெச்சி இருக்கேன் அதுல பொய் உம்மா குடு, உன் பூலை வெச்சி தேயி, பாடு.',\n",
       " 'இவர் வந்தாலே லைக்ஸ்லாம் தெரிவிக்கும் டெம்ப்லெய்ட்ஸ் எல்லாம் பறக்கும். \\n\\nமெஸஜ் ஸ்பெலிங் என்னன்னு சொல்லு ஆக்ஸ்போர்டு இவர் கிட்ட தான் கேட்டு தெரிந்துகொள்வோம்.\\n\\nஅவன் தன் மலத்தில் ஸ்வாக் கிடைத்தது அப்படிப்பட்ட ஒருத்தர் தான் இன்று நம்மளுடைய நிகழ்ச்சி வந்திருக்கிறார்.\\n\\nஜி.வி.எம்.ஸி அப்படி! என்று சொல்லி நம்ம சொல்லிருந்தேன் ஜி.வி.எம்.ஸி.கணேஷ் விஸ்வநாதன் ம்ஸி  வணக்கம் ஸொவ் வணக்கம் \\nஐயா என்ன எம்.ஸி\\n\\n\\nமாதர்ஷோட் அப்படி என்று எல்லாம் கெய்ட் பண்ணலாமா இல்லை முட்டாள் சுன்னி என்று னாட் லைக் தட் அப்படி பார்த்தாலும்\\n\\n\\nசுன்னி க்கு எஸ் தா நா வரும் இல்லை ஐயா நாம சி போடலாம் நம்ம யார் வந்து கேட்க வேண்டும்?\\n\\n\\nமுட்டாள் சுன்னி டான்ட் லவ் தட் இஜ் அ.\\n\\nஎம்.ஸி மிம் கிரேட்டர்  கணேஷ் த மிம் கிரேட்டர்  \\n\\nவந்து நீங்க எனக்கு ஒரு இன்றோ  கொடுத்து ஸ்டார்ட்  பண்ணிங்கன்னா நல்லாயிருக்கும் உங்க ஸ்டைல் எனக்கு ஸொவ் \\n\\n\\nதிஸ் ஐஸ் ஷோ விளக்கு பிடி ஸ்பொன்சர்ட் பை தா தேவிடியா\\n\\nசாரி கிரல்ஸ் லேடீஸ் வோமேன் லாம் பாத்துட்டு இருகாங்க நா இல்ல சார் நாம பீப் போட்டுக்கலாம் நீக்க அப்டியே திட்டினாலும் பரவா இல்ல. \\n\\nஅப்டி இருந்தாலும் நா அப்டி பேச மாட்டேன். சோ லேட் இட் பெ திஸ் வாய் திஸ் ஐஸ் எ ஷோ விளக்குப்பிடி வ்திச் ஐஸ் ஸ்பொன்சர்ட் பை சினிமா பய ',\n",
       " 'சார் நீக்க வரத்துக்கு முன்னாடி சொல்ட்டு இருந்தேன் கெட் ரெடி டு பால் இந்த லவ் வித் கெரிஞ்சு. \\nஅந்த கெரிஞ்சு நிறது எப்படி பாக்குறாங்கனு தெர்ல பட் யு ஆர் மேக்கிங் திஸ்  அஸ்பெக்ட் ஆப் டால்க்கிங்  டு ஏ வந்து இம்மெர்சிவே கெரிஞ்சு ஆ மாறிடுது பட் உங்க கிரேட்டிவ் மீ ம்ல பாக்கும்போது எனக்கு ஒன்னு தோணுச்சு \\nகொஞ்சம் வந்து லைட்  ஏ அண்டர் ரேட் பண்றோமோ னு சொல்ட்டு.\\n இ லைக் டு எஸ்பிளான் தட் மீ ம் ஜொனி சீன்ஸ் ஓடி டிக்  போட்டோ ஒன்னு போட்டு அது ரொம்ப பியூட்டி பியுள்ள பழக் அண்ட் வைட் ல போட்டு ஒரு டெஸ்ட் ஒன்னு போட்டு இருந்தீங்க. \\nநா வீண்வேன் என்று நினைத்தாயோ இல்ல எஸ். எங்குமே அந்த பீல் இருந்துச்சு ஆக்சுவல் ஆ ரொம்ப ரசிச்சு ஒரு கான்செப்ட் ஒன்னு புடிச்சு அத வந்து டேம்ப்லடே கூட மேர்ஜ் பனி ஒரு பைனல் வுட்புட் ஆ ஒரு மீ ம் ஆ பாக்குறப்போ  \\nஒரு ஹார்ட் பிரேக்கிங் னு சோலா கூடாது அது ஒரு ஏலியன் லெவல் ல ஒரு மீ ம் ஒன்னு பனி இருக்கோம் சொல்லி நெனச்சுட்டு இருப்போம் பட் இட் டிட்ய்ண்ட் ஹிட் மோர் தன் பைவ் லைக்  அதுமட்டும் இல்லாம  இப்ப பியஸ்  புக் நால ஏக ஏக ஏக.\\n வித் ஆல் டூ \\nரெஸ்பெக்ட்  இ டோன்ட் மைண்ட்  சாயிங்  திஸ் பியஸ்  புக் கம்யூனிட்டி  ஸ்டாண்டர்ட் லெவல் ல அது இல்ல னு சொல்லி அத ரிமோவ் பண்டங்க இன்னும் கொஞ்சம் பூஸ்ட் பனி ப்ரோமொடே பனி இருக்கலாமோ\\n அப்டி எனக்கு தோணுச்சு இன் கேஸ்  அப்டி நடந்து இருந்தது நா இ வில் பீ  எ கிரேட்டிவ் மீ மர். ',\n",
       " 'இப்போ இன்ட்ரெஸ்ட்டிங் ஆனா ஒரு டாஸ்க் ஓகே எனக்கு கி வ ம் சி ட ல் எ பிலீஸ்ர் பிளான்டர், வாட் எவர் யு கால் இட் எனக்கு அதெல்ல ஓடையானும் மோதலை \\nஇந்த கால் கிராஸ் லைன் போடாதீங்க எடுங்க \\nஏன் யு வான கிவ் ஹெட் ம  இ மீன் ஊம்ப போறீங்களா. \\nஎனக்கு  அந்த ஆசை இருக்கு பட் இப்ப வேண்டா.\\nஇப்போ எனக்கு யுங்க ஜட்டி அத மட்டும் கழட்டி குடுத்தீங்க நா பொது. \\nகைஸ் வி ஆர் கிரீட்டிங் ஹிஸ்டரி ஜி.வி.எம்.ஸி சோசியல் எம்.ஸி ஓட ஜட்டி நம்மகிட்ட இருக்கு.\\nஏன் போட்டு பாக்கப்போறீங்களா \\nஇல்ல சார் நா இத சினிஃ பணவச்சுக்குறேன். \\nஜி.வி.எம்.ஸி நா அழகு தா னு இருக்கு எப்போதுமே வந்து சோஃபிஸ்டிகேட் பிரோதெரஸ் வித் க்ரின்ஜ் கிரஷ் டயபர் சினிஃபைங்  அது தா இருக்கு. நாம ஏன் அழுக்கு  பத்தி பேசக்கூடாது நீங்க ஏதாவது வந்து திரட்டி ஆ பண்ணனும் எங்களுக்காக. \\nசரி வந்து குனி.\\nஆ காண பாடுறீங்களா சி இங்க ஒரு ஜாம் செஷன் போடுங்க லோக்கல் அஹ்ஹ்ஹ ',\n",
       " 'சமீபமா நெறய ரேபீஸ் லாம் நடக்குது வாட் டூ யு ஸ்டாண்ட் ஆன் தீஸ் இஸ்ஸுஸ் \\nசி தாட் இஷ் எ குட் கொஸ்டின் அச்சுஅல் ஆ அந்த எனக்காவுன் எ அப்ளவுட் பண்ணனும். \\nவி நௌன் டு தி கோக் தாட் இஷ் எ ஹெரோய்க் மொமெண்ட். \\nஆமா சார் பட் அடுத்த நாளே யு போஸ்டேட் சம்திங் அந்த ௨௦௦௦ ரூபா ப்ரா வந்து ௨௦௦ ரூபா தா அந்த பொன்னுலம் ஏன் ரேப்  பண்ணக்கூடாது னு சொல்லி போட்டுஇருந்தீங்க \\nசோ அந்த மொமெண்ட் ல உங்களுக்கு சோறு தா தின்கீரையா இல்ல பீ அல்லி தின்கீரய டா பே புண்ட மவனே இந்த பொழப்புக்கு நாலுபேரு பூலை ஊம்பி பொழைக்கலாமே டிவோலோ பயா அப்டினு. \\nசி சி \\nஅப்டினு சொல்லி உங்களுக்கு வாய்ஸ் ஓவர்  எதுவும் கேக்கலையா சார். \\nசி சி இந்த எடத்துல தா நீக்க வந்து புருஞ்சிக்கணும் அஸ் எ மேன் இ ரெஸ்பெக்ட் வோமேன் பட் ப்ரா ஸ்ட்ராப் பாத்தேழாம் இ கெட் போனீர். \\nப்ரா இஷ் எ கன்சன். அதுனாலதா அந்த போனுக்கு ஒரு பதிலடி கொடுக்கணும்னு அந்த மீ ம் ஒன்னும் பனோம் ப்ரா ஸ்ட்ராப் போட்டா திஸ் கேர்ள் ஷட்  பீ ரேப்ட் அப்டினு சொல்லி ஒரு மீ ம் ஒன்னு போட்டோம். \\nபட் அல்டெர்நேட்டிவ்ல்லி ரேப் கு ஒரு சொலுஷன் வேணும் அப்டின்றது மை ஒப்பீனியன் .\\nஓகே சார் அந்த ரேப் கு வந்து என்ன சொலுஷன் ஆ இருக்கலாம் ஜஸ்ட் ஒரு சஜஸன் சொன்னீங்கனா ',\n",
       " 'ஏன் டா நீயே உன்ன ரேப் பங்கிட்டு இருக்கையே. நா சொன்னே மச்சா இது ஒரு சீன் காக நா ரீஹர்ஸ் பண்ணீட்டு இருக்கேன் அந்த பொண்ணு அத ஒதுக்களை னு மட்டும் தா லவ் அ ஒதுக்களை நா மட்டும் தா அதெல்ல நடக்கும் அப்டினு நா சொல்லி இருந்தேன். அப்டி ரேப் பண்ண முடியலைன்னா இ வில் மேக் அ மீ ம் அவுட் ஆப் ஹேர். இ அம் ஸ்லுச்சிங் ஹேர் தட்ஷஹௌ இட் ஒர்க்ஸ் அண்ட் ஷி பால் இந்த லவ். டெல் அஸ் அபௌட் தி மீ ம் செரிஸ் யு ஆர் பிளானிங் போர் வெரி லாங் பீரியட் ஆப் டைம். யாரு கிட்டயும் சொல்லாம ஒன்னு பண்றீங்களாமே அது என்னனு சொல்லி ஷேர் பணிகொங்க பட் ஆடின்ஸ் பாத்துட்டு இருக்குறவங்களாம் இது ஒரு புதுசா ஆகுமெண்ட்டட் ஆ ட்ரை பேணுவோம் எல்லாரும் வந்து கண்ண மூடுங்க. இட்ஸ் அ ஆட்டோ பயோ கிராபி பிக்சன் காமலக்கண்ணன் காமாச்சி அப்டினு ரெண்டு பேரோட கதை அ எழுதிட்டு இருக்கேன். மீ ம் கிரேட்டர்ஸ் சோசியல் மீ ம் அர்ஷ் இவங்க ரெண்டுபேருமே மீ ம் பேஜ் ல கமெண்ட் ல மீட் பண்றங்க. தே கனெக்ட் அக் ராஸ் கிரிங்கே அக் ராஸ் விவசாயி டிக்ஸ் , அக் ராஸ் துப்பட்டா போடுங்க தோழிஸ் பட் லெகின்ஸ் போடாதீங்க கான்செப்ட் இந்த மாறி விஷயங்கள் ல. வி ஈவென்ச்சுவல்லி மீட் அப் டு பைன்ட் போது ஆப் தெம் ஆர் கைஸ் பட் நன் ஆப் தெம் ஆர் கெய்ஷ். ',\n",
       " 'பட் நன் ஆப் தேம் ஆர் கெய்ய்ஸ் லேட் தே டிஸைட் லிவ் டுகெதர்,\\nபிகாஸ் தே நெவெர் கெட் எ பார்ட்னர்.\\n நோ மேட்டர் வாட் தெயர் செஸ் வல் ஒரிஇண்டஷன் ஈஸ்.\\nபிகாஸ் ஆல் தே காட் ஈஸ் ஷீட் இந்த தி ஹெட். \\nஇதுக்கு டெம்ப்ளட்ஸ் கலெக்ட் பண்டு இருக்கேன். \\nஇப் ஐ கெட் யணப் இட் வில் பீ ட்ரூத்புல் மீ ம் சீரிஸ். ',\n",
       " 'இதுல மாக்ஸிமும் நாம கூட இருக்குற பசங்க எப்படி தெரியுமா நா படிக்கல படிக்கல படிக்கல படிக்கல னு சொல்லி நம்மள வீட் வச்சு ட்ரிப் ஆகி இருக்கும் பொது படிச்சு ஆல் கிளியர் பண்டு போயிட்டே இருப்பாங்க அறிவுசான் திருட்டுப்புண்டைக.\\nவீட் குடுக்குற எக்ஸிஸ்டன்ஸ் புக் குடுகுமா கண்டிப்பா கிடையாது அப்டி இருக்கும் பொது ஒரு தக் வீட் 250 னு இருக்கும்போது \\nஒரு ஆரியர் காண பீஸ் 300 ரூபீஸ் யாரு முட்டாள்.\\nநா உங்க கிட்ட ஒன்னு கேக்குறேன் 3 மணி நேரத்துல ஒருத்தன் அறிவாளி முட்டாள் னு தேர்வு பண்ண முடியுமா \\nஇந்த 3 மணி நேர டெஸ்ட் ல 9 எடுத்தவன் அறிவாளி அப்போ 8 எடுத்தவன் அவனை விட அறிவு கம்மி 7 எடுத்தவன் அவனை விட அறிவு கம்மி.\\nஅப்போ ரோல் பண்ண ஓசி பிக் கிடைக்காம எக்சாம்க்கே வராதவன்,\\nஎக்ஸாம் ஹால் ல வைட்னர் எ ஸ்லிப் பண்ணி ஹை ஆன அந்த நாயி,\\n ஹான்ஸ் வச்சுட்டு பாதி அன்சார்  எழுதாம வரானே அந்த நாயி முட்டாள். ',\n",
       " 'முட்டாள் யாருக சொன்ன இதெல்ல பாக்ட்டுன்னு \\nஅக்சுஉவள் அ என்ன பொறுத்தவரைக்கும் யாரு அறிவாளி னு தேர்ந்தெடுக்குறது இல்லைங்க யாருக்கு சுத்துக்கொழுப்பு அதிகம் னு தேர்ந்தெடுக்குறது\\nஒதுக்குறேன் க எக்ஸாம் கே வராம எக்ஸாம் கு அரியர் வச்ச அந்த பசங்க 3 ஹவர் ல தங்களுக்கு சூத்துக்கொழுப்பு ஜாஸத்தி அப்டினு புரூப்  பண்ணிட்டாங்க னு ஒதுக்குறேன் \\nஆனா லைப் வெறும் 3 ஹவர் இல்லைங்க இதுக்கப்றம் எத்தனையோ வருஷங்கள் இருக்கு \\nஅவன் லைப் புல்லா ஓல் படணும் அப்பதா புத்தி வரும் அந்த தாயோலிக்கு \\nலைப் ரொம்ப அழகானதுங்க லாஸ்ட் பெஞ்ச் ல உக்காந்து ஹான்ஸ் வச்சுட்டு இருந்தவங்க எத்தனையோ பேரு இன்னைக்கு பெரிய ஹான்ஸ் சபிள்ளையார் ஆகி இருக்காங்க \\nஅத  உங்களால மறுக்க முடியுமா மருக்குறவங்கள விட பெரிய முட்டாள் வேற யாரும் இல்லைங்க ',\n",
       " 'லைப் ல ட்ரிப் ஆகி சிறுச்சவங்கள விட வீட் கிடைக்காம அழுத்தவங்க தா ஜாஸ்த்தீ \\nஐ நோ தி பெயின் ஆப் தி பெலியர்ஸ் பிரண்ட்ஸ் ஒவொரு தடவ அரியார் வைக்கும்போது ரிசல்ட் பைல் னு போடும்போது தீபார் னு போடும்போது\\nஓத்தா இதுயென்ன சப்ஜெக்ட் னு பேறே தெரியாத சப்ஜெக்ட் ல பைல் ஆகும்போது வர அந்த பெயின் காப் சிரப் குடுச்சாலும் போகாதுனு \\nஎனக்கு தெரியும் இப்படி நீ எவளோ தோத்து இருந்தாலும் நீ மட்டும் விட்டு குடுத்துடாத உண்மையான தோல்வி எப்ப தெரியுமா நா தொட்டுட்டேன் னு\\n நானே எப்ப டிக்கலர் பணிக்குறேனோ அப்பத்தா இன்னும் நாலு வருஷம் அரியார் எழுத்து ஓத்தா  யாரு கேப்பா \\nவீக்கான சப்ஜெக்ட் கு டியூஷன் பொய் டீச்சர் அ கரீக்ட் பண்ணு ஒம்மலே யாரு தடுப்பை \\nஇந்த உலகம் உன்ன 1000 தடவ தோக்கடிக்கலாம் இந்த சமூகம் உன்ன லச்ச தடவ தோக்கடிக்கலாம் \\nயாரு தொகடுசலும் போடா புண்டை அப்டினு சொல்லி வீட் அடுச்சு போயிட்டே இரு ',\n",
       " 'ஆமா பெயிலு அரியார் விழுந்துடுச்சு படிச்சு எக்ஸாம் எழுதி அரியார் விழுந்து இருந்தா கூட பரவா இல்ல ஒம்மலே கூதி கொழுப்பு நாலா எக்ஸாம் கே போகாம அரியார் இப்போ என்ன பண்ண போற டெய்லி உம் எக்ஸாம் காலேல ஒரு எக்ஸாம் ஈவினிங் ஒரு எக்ஸாம் அதுலயும் பெயில் இப்போ நியாபகம் வச்சுக்கோ தோல்வி ஒரு வரம் மாறி தோத்து பொதுக்கப்றம் குடுக்குற சரக்கு தா கிக் அதிகம் தோல்வி வர அப்ப அத வச்சு எப்படி கிக் ஏத்தலாம் னு நினைக்குறேன் ல அவன் தா பெரிய ஆளு இங்க ரெண்டே சாய்ஸ் தா பிரிஎண்ட்ஸ் ஒன்னு முதுகை காட்டிட்டு ஓடுறது வாழ்க உன்ன வேரடி வேரடி ரேப் பண்ணும் இன்னோனு நின்னு பேஸ் காட்டி வாயில வாங்குறது உனக்கு புடிச்சாலும் புடிக்கலைனாலும் வாழ்க உன்ன புரட்டி பூட்டு ஓக்கும் டா எப்போ புக்ஸ் அ பெரிசா பாக்குறையோ அப்போ அது உன்ன அடிமை ஆகி உன்ன தோக்கடிச்சு உன்ன அழுச்சிடும் எப்ப அரியார் விழுந்தா மைராச்சு நா ஹை ஆகதா போறேன் கெளம்பிபோரையோ இந்த டிகிரி இல்ல எந்த செர்டிபிகாடே கோர்ஸ் உம் உன்னால தேறமுடியாது. ',\n",
       " 'வெள்கம் டு பிலிப் விமர்சன நிகழ்ச்சி இன்னக்கி நாம பாக்கபோறது ரொம்ப கேவலமான தரிதுரபுடிச்ச பாதிலே சூடாக்குற ஆனா அது கிட்ட நெருங்க முடியாத ஒரு கேரக்டர் பத்தி தா பாக்கபோறோம் இன்னும் சோல போனா ரிலேஷன் ல இருக்குற எல்லா படங்களுமே எப்ப ட நாம ஆளு நம்மள விட்டுட்டு போவா அப்டினு பயத்தோடு சாமானை புடுச்சுடாதே திரியுறதுக்கு காரணமா இருக்குற ஒரு கேரக்டர் கேர்ள் பிரென்ட் ஓடிஏ பெஸ்ட் பிரென்ட் பசங்க பொண்ணுக ரெண்டு பேருமே பண்ணாலும் கேர்ள் பிரென்ட் ஓட பெஸ்ட் பிரென்ட் பயனா இருந்தா சுவாரசியமா இங்கும் ர நால அவைங்களா பத்தி பாக்கலாம். மொத மொத இந்த கருமந்த்ர லவ் அ ஆரமிகுரோமோ அப்போவே அவங்களோட குடும்ப நடத்த பழகிக்கணும் ஏனா நாம கேர்ள் பிரென்ட் கூட ஓடிட்டே தெரியுவாங்க இப்போ வந்த படத்தில்கூட ஆறா னு சொல்லிட்டு நாம உடம்ப சுத்தி ஏதோ தவறு இருக்குனு சொல்லி ஓல ஒத்து இருப்பாங்க இவனும் திட்ட தட்ட நாம ஆளோட ஆரா மாறி தா இவங்களுக்கு தெரியாம எந்த முடிவும் எடுக்க முடியாது. ',\n",
       " 'படத்துக்கு போலாமா னு கேட்டா இந்த படம் நல்லா இல்லனு சொல்வாங்க சேரி படத்துக்கு வென பீச் கு போலாம் அப்டினா இன்னக்கி மல வரமாரி இருக்கு அப்டின்வாங்க சேரி மல வர மாறி இருக்கு வீட்டுல யாரும் இல்ல வரையா னு கேட்ட அம்மாவாசை அணுகி குடக்கூடாது அப்டின்கிறாங்க புண்டாமவனுக அவளுக்கு எப்ப டெட் வரும் னு தெரிஞ்சு வச்சுக்கிட்டு அன்னக்கி தா அவளை வெளியவே விடுறாங்க ஏன்டா இந்த மானக்கேட்ட பொழப்பு ஈதரை பயலுகளா அப்டினு கேக்க தோணும் சேரி தனியா போறனாள தா சாதி பண்றங்கனு சொல்ட்டு அவங்களையும் கூட்டிட்டு போனா அவங்களுக்கும் சேது நாம தா மோய் எழுதணும் கொடுத்த காச திருப்பியா கேட்கமுடியும் அவ கூட படுத்து வென கழுச்சிக்குளம் பிரச்னை என்னனா சாப்பிட கூடுபோனா வாய பாக்குறாங்க னு சொல்ட்டு ஐஸ் கிரீம் சாப்பிட போனா அங்கேயும் வாய வாய பாக்குறாங்க னு சொல்ட்டு அவனுக்கும் வாங்கி கொடுப்போம் அதே மாறி தியேட்டர் கு பொய் முத்தம் குடுக்க போன அங்கேயும் வந்து வாய வாய பாக்குறான் இந்த சந்திக்கூதியை என்னனு சொல்றது. ',\n",
       " 'மொதோலாம் லோவெர்ஸ் கூட ஏதாச்சும் பிரச்னை வந்தா ஒரு ஒரு வாரம் பத்து நாலு பேசாம இருப்பாங்க. ஏன் நா ஒருத்தரு இல்லாம ஒருத்தரு எப்படி காஜுல தவிக்குறாங்கனு உணரத்துக்காகதா அப்பளம் இந்த மாறி கேரக்டர் இல்லனு தா சொல்லணும் அப்டியே இருந்து இருந்தாலும் இவங்களுக்கு லோவெர்ஸ் குலா இருக்க பிரச்னை என்னனு தெரிஞ்சு இருக்காது ஆனா இப்ப டெக்னாலஜி வளர வளர இவங்க என்ன பன்றாங்க இனம் பெருக்கம் பன்னி வாழ்த்துட்டாங்க னு தா சொல்லணும். எப்ப அவங்களுக்கு குல சண்டை வரும் துண்டை போடலாம் னு காத்து இருக்காங்க புண்டமவனுக. அவ வாட்ஸாப்ப் ல சோகமா ஏதாச்சும் ஸ்டேட்டஸ் வச்சுட்டா போதும் நீ ஏன் சோகமா இருக்க நீ சிரிச்ச அழகாகிடும் னு சொல்லி வந்துடுவாங்க பெத்தவங்கள கூட இவளோ பாசமா பாத்து இருக்கமாட்டாங்க அவளோ அன்பு இதே சம்பவம் நேர்ல நடந்துச்சுனா கிறுக்குபுண்டை மாறி வித்த காமிச்சு அவளை சிரிக்க வச்சுடுவாங்க. இந்த கிறுக்குத்தனத்தை பாத்துட்டு ஓரமா போடா சுன்னி அப்டினு சோனா அவளுக வந்துட்டு அவன் எவளோ ஸ்வீட் அத் நடந்துக்குறான் உனக்கு அவன் மேல பொறாமை அப்டினுவாக இந்த கிறுக்குக்கூதி தனத பாத்து நாம பொறாமை வேற படணுமாக்கோம். ',\n",
       " 'ஏற்கனவே சந்தோஷ்சுப்ரமணி படத்துல வர காசினி மாறி திரியுறாளே. இவன் பண்ற மொக்க காமெடி குலாம் சிரிக்குறளே னு நாம பரிதாபம் பட்டோம்னா நம்மள பொறாமை படுறான் னு சொல்றளுக. அவங்கள சாதாரணமா சொல்லிட முடியாது அவன் நாளாவே ஓல ஓப்பான். அவனுக்கு விஷபேர் இல்லனாலும் கூட னால துல்லியமா அர்த்தம் சொல்லிகுடுப்பான் எந்த அளவுக்கு துல்லியமா நா சில நேரம் புறநானுறு சிலப்பதிகாரம் வெரிகுட் கூட பொய் கோட் பனி அர்த்தம் சொல்லி குடுப்பான். நாம கோவத்துல ஏதாவது ரெண்டு கேட்ட வார்த்தை போடு திட்டிட்டேன் நா இது மோதலை அவனுக்கு போகும் அவன் தனி தனி வார்த்தைகளா பிரிச்சுடுவான் அப்பறோம் ஒரு ஒரு வார்த்தைக்கும் துல்லியமா விளக்கிடுவான். ஒடனே அவளு உன் மனசுல நீ என்ன இப்டிதா நினச்சுட்டு இருந்தாயா அப்டினு நாம கிட்ட கேப்பா. ஏன்டா சுன்னிகளா கோவத்துல ஏதாச்சும் ரெண்டு எது எதுனா அது கூவம் மட்டும் தா வார்த்தைக்கு அர்த்தம் கண்டு பிடுச்சுட்டு இருந்த நாம என்ன பண்றது னு கேட்டா அவ அதையும் பொறாமை னு நினச்சுடுவா னு சொல்லிட்டு நாம அப்டியே போயிடுவோம் இந்த எளவுக்கு பிஎப்எப் னு பெரு வேற நாம கடைசி வர பிரண்டு னு சொல்லி கேப்ஷன் வேற வைப்பாங்க ஏன் பிரெண்டு னு சொல்லிட்டு நீயே அவளை வச்சு வாழறா கூதி னு நாம கிளம்புனா நம்மள கெட்டவன்ராலுக. ',\n",
       " 'ஊம்பு காதலாம் கொடுக்கமுடியாது டாய் லிவ் போடு இருக்கு டா சாரி. ஹலோ ஆல் இது உங்களுக்கு பிடிச்ச ஷோ காலுக்கு நடுவில் அப்டினு உங்க எல்லாருக்கும் தெரிஞ்சு இருக்கும். உங்க காதலை வெளிப்படுத்த நினைச்செங்கணா நா ரெடி அத் இருக்கேன் நிகழ்ச்சி ஓடிஏ முதல் காலர் லிவ் ல இருக்காங்க ஹலோ ஹலோ சார் ஹலோ ஹெலோ ஹலோ உங்க டிவி வால்யூம் அ கமி பண்ணுங்க ஏன் டா புண்டை என்ன லந்து குடுத்துட்டு இருக்கியா டிவி வால்யூம் முட்ட பண்ணுடா பாடு. உங்க பெரு சோனா னால இருக்கும் தங்கராஜ் பேசுறேன் சார் தங்கராஜ் உங்க வாய்ஸ் கேக்கும்போதே நீக்க காதல் இருக்கீங்கன்னு சொல்லி ஐயோ எப்படி சார் கண்டுபிடிசீங்க. பின்னாடி பாக்கிரௌண்ட் எப்படி இருக்கு தங்கராஜ் நல்லா இருக்கு சார் பின்னாடி க்ரீன்மாட்டே செட் போனது அந்த புண்டைக்கி தா. அதுக்கு முன்னாடி நா நிகழ்ச்சிக்கு போறதுக்கு முன்னாடி நா ஒரு இண்ட்ரோ கொடுத்து இருப்பேன் ஒரு கரடி கற்பழிச்சு கா கா அப்டினு சொல்லி அதுனால காதல் ல இருக்குறவங்க மட்டும் தா கால் பண்ணனும் நீங்க லவ் பந்தள அப்டினு சொன்னீங்கன்னா ஓத்தா ஒம்மா அப்டினு சொல்வேன் வீடு புகுந்து வெட்டுவேன் அதுனால நீக்க லவ் பண்ற பொண்ணு பெரு மட்டும் சொன்னீங்கன்னா நல்லா இருக்கும் தங்கராஜ் பூங்குழலி சார் அவளை நா உயிருக்கு உயிரா லவ் பண்றேன் அவளுக்காக நா ஒரு கவிதை கூட எழுதி இருக்கேன் கட் பண்ரா கட் பண்ரா. ',\n",
       " 'கட் பண்ரா புண்டாமவன் எடிட் ல போயிடும்ல. சாரி தங்கராஜ் உங்க கால் ல எதோ பிரச்னை னு நினைக்குறேன் அடுத்த தடவ கால் பண்ணுங்க நாம பேசலாம் அப்புறம் உங்களுக்காகவும் உங்க லவர் காகவும் ஒரு வீடியோ போடுறோம் பாத்து என்ஜோய் பண்ணுங்க. எல்லாருக்கும் வணக்கம் நாம சாதி காரங்க லாம் நல்லா இருக்கீங்களா. நா சாதி பத்தி பெருறேன் னு நினைக்காதீங்க நாம ஒன்னு சாதாரண இனம் கிடையாது நாம சாதி ல எவளோ பெருமை இருக்குற நாளந்தா நாம இவளோ பேசுறோம். ஆசியா லே எங்க கட்டு பாட்டுல வச்சுஇருந்தவங்க நாக. ராஜாக்கள் தமிழ்நாட்டுல இருந்தப்போ அதன்பெருக்கு ஊம்பி விட்டுட்டு இருந்தவங்க நாங்க சாதாரண பரம்பரை கிடையாது. இந்த ராஜா பசங்க எல்லாருமே எங்க சுகத்துக்கிட்ட அடிமையா இருந்தவங்க. நாங்களும் சுகத்தை குடுத்துட்டு இருந்தவங்க எங்க கொள்ளுத்தாத்தா வாயிற் வெங்கடேஷு சும்மா இல்ல சல்ட்டு செபாசிலிஸ்ட் உப்பு போடாம தப்பு பண்ணாதே கிடையாது. எங்க தாத்தா பெரியமீசை பிரகாஷு சல்ட்டு சுவீடு ரெண்டும் போடு சுகம் கொடுப்பாரு. ',\n",
       " 'எங்க அப்பா வீர வெண்கொடுக்கு சல்ட்டு சுவீடு லெமன் னு சுகம் கொடுத்தவாறு அப்படிப்பட்ட பெருமையான எனத்துல பொறந்தவங்க நாம அதவாது விட்டு கொடுக்காம இருக்கனும். இந்த நுணுக்கம் கலை பெருமை எல்லாம் நம எனத்துக்கே சொந்தமானது அத நம பெருமையை சொல்லி தா ஆகணும் அத நாக சொல்லிட்டே தா இருப்போம். ப்ரெண்ட்ஸ் இது வந்து நம சொந்தகளுக்காக நம வீடியோ கு லைக் ஏ வரமாட்டிங்குது ஏன்னு தெர்ல இந்த டிக் டொக் காரங்க நம பெருமைய எல்லாம் இந்த உலகத்துக்கு தெரியக்கூடாது னு சொல்லி மொடக்குறான் னு நினைக்குறேன் நம அத முறியடிச்சு காமிக்கும் எல்லா சொந்தகளுக்கும் ஷேர் பண்ணிட்டே இருங்க. ஆங்கிலேயர் எப்படி நம கிட்ட சுகம் போதும் போதும் னு சொல்லி ஓடுனாங்களோ அதே மாறி நம ஓடிஏ விடுறோம் தெறிக்க விடுறோம் ப்ரெண்ட்ஸ். நம ஓம்சு காட்டு படை னு சொல்றது சங்கடமா நினைக்கக்கூடாது நண்பர்களே பெருமையை நினைக்கணும் ஊம்பு கட்டு படையின் சார்பாக நன்றி  சொந்தங்களே. ',\n",
       " 'அடுத்த கலர் எங்க இருந்து பேசுறீங்க. சார் நா பாபு பேசுறேன் சார் நா உங்க பெரிய பேன் சார். நா உங்க கிட்ட ரொம்ப நாலா பேசணும் னு ட்ரை பண்ணிட்டே இருந்தே லைன் ஏ கிடைக்கல எப்டியோ லைன் கிடைச்சுடுச்சு எங்கு என்ன பண்றதுனே தெர்ல சார் ரொம்ப சந்தோசமா இருக்கு சார். இத ஒரு பொண்ணு சொல்லி இருந்த நா ரொம்ப சந்தோச பட்டு இருப்பேன் பாபு நீ என்ன புண்டைக்கு இவளோ ரசிக்குறீங்க எனைய. எனக்கு உங்கள ரொம்ப பிடிக்கும் நா டெய்லி உங்கள டிவி ல பாப்பேன் சார். டாய் உண்மையா சொல்லடா. சார் ஆபத்தான கேட்ட பாட்ட போடுவீங்க. வயசு என்ன ஆகுது பாபு ௨௬ சார் நீக்க பிளாஸ்டிக் சைல்ட் இல்ல உங்க ஹெஅழ்த் ஈசு ஏதாச்சும் இருக்க உங்களுக்கு. ஐ ஆம் வெரி குட் அடல்ட் சார் மழுமுத புண்டை இவளோ பேசுரேலை உனக்கு புடுச்சி பாட்ட டவுன்லோட் பண்ணி போகவேண்டியது தா ந ட மயிறு. இது ஒரு நிகழ்ச்சி ரொம்ப நாலா வெயிட் பண்ணி கூப்டு பாட்ட வேற கேக்குற அறிவு மேரு னு இருக்க இல்லையா ட உனக்கு. உண்மையா சொல்லு உன் வாட்ஸாப்ப் டிபி கருப்பு ரிப்பன் தா ந ஆமா சார். ',\n",
       " 'பலான டாக்டர் பெரிய பிராட் தாயோளி மீம் கிரிஎட்டர் ரெவிஎவ் கு நடுவுல பொம்ம விக்கிறவன் பெரிய கஞ்சா சப்ளையர் ஹெஸ் பிசினஸ் பண்றவன் சாமியார் தமிழ் ராக்கர்ஸ் ஐட்டம் புண்டை சங்கிப்பயன் பார்ன் ஆக்டர் பார்ன்  ஆக்டர் அவங்க அடுச்சங்க ந நம தாங்க மாட்டோம் துணையும் சேது தா. ',\n",
       " 'இந்த வீடியோ வ பாத்துட்டு இருக்கும் போதே கண் கலங்கி இருக்கும் அதுக்குள்ள காலர் வந்தாங்க ஹலோ யாரு பேசுறீங்க. சார் ஏன் பெரு ஹரிஹர சுதன் பிரேம் வளசரவாக்கம் சென்னை சார். ஹரிஹர சுதன் உங்கள கேன சுதன் னு கூப்பிடலாமா. ஓகே சுதன் உங்க வாய்ஸ் ஏ சூப்பர் ஆஹ் இருக்கு உங்க லவ் ஸ்டோரி அ பத்தி சொல்லுங்க. சார் ஐ ஆம் அகைன்ஸ்ட் லவ் சார். அப்போ எதுக்கு நிகழ்ச்சிக்கு கூப்பிடுறீங்க சுதன். பட் ஐ லவ் சம்ஒன் சார் அவங்கள பத்தி பேசலாம் னு தா கால் பன் சார். உங்க காதல் கதை பத்தி கேக்க குரியஸ் அ இருக்கேன் சொல்லுங்க. நா ஒரு போனோட அப்பனா பேசுறேன் சார் ஒரு குழந்தையை பெத்து வளக்குறது எவளோ கஷ்டம் னு தெரியும். நம வளத்தா ௨௫ வயசுல எவனோ ஒரு பரதேசி வந்து தட்டிட்டு போயிடுறேன் சார். ',\n",
       " 'உங்களுடைய கோவம் வந்து எனக்கு புரியுது எப்படி ஒரு போன ௨௫ வருஷம் வலது இன்னோருத்தவன் போட விடுறது அப்படின்ற கோவம் வந்து உங்க கேள்வி இருந்தது. பேசாம நீங்களே போட்டுருங்களே. பங்கு சூப்பர் பங்கு. எனக்கும் உங்க கிட்ட பேசுனது ரொம்ப சந்தோசம் இனிமே உங்க பேர் என்னனு கேட்ட இன்ஸஸ் சுதன் னு சொல்லணும் அடுத்த தடவ இன்ஸஸ் சுதன் னு சொல்லுங்க நா கரெக்ட் அ ஐடென்டிபிய பண்ணிடுவேன். இவானா மாறியே எல்லாரும் இன்ஸஸ் அ இறந்துட்டாங்க அப்டினா காதலும் இருக்காது காதல் சார்ந்து வர ஆணவ கொலையும் இருக்காது. ',\n",
       " 'வணக்கம் ஆண்கள் ஆண்கள் எல்லாரும் சம்பாரிக்காம இருக்கலாம் ஆனா எல்லோரு கைகளிலும் கம்பு இருக்கும் அறம் செய்ய அஞ்சாதவங்க ஆண்கள் மொல செய்ய துணிந்தவர்கள் ஆண்கள். ஆண்களுக்கு ஏழாம் இருக்கிற ஒரே பயம் தா இறந்த பிறகு தன ஆளை எவன் போடுவான் என்றுதான். முறைத்த பேனை துரத்தி பிடிப்பவர்கள் ஆண்கள். விடாமல் ஒம்மலே ஆசிட் அடிப்பவர்களும் ஆண்கள். அளவுக்கு அதிகமாக அன்பு செலுத்துபவர்கள் ஆண்கள் அனாவசியமாக ரேப் செய்பவர்கள் இல்லை ஆண்கள். தேவை பாடல் எம்பட்டி பஸ் இல் தொடையில் காய் வைப்பாரும் ஆண்கள். இருட்டில் பயம் அறியா இருப்பவர்கள் ஆண்கள். ஒருவனுக்கு பெண் தோழிகளை விட ஆண் தோழர்கள் இருந்தால் அவன் ஆண்களை போடும் காமுகனாகவும் இருக்கலாம். ',\n",
       " 'இந்த நிகழ்ச்சி ஓடிஏ லாஸ்ட் செக்மென்ட் கு வந்து இருக்கோம் யார் அன் லாஸ்ட் அண்ட் லக்கி காலர். சார் நா உங்க கிடையா பேசுறேன் நீங்களா இது சார். ஆமா நீக்க ஏன் கிட்ட தா ஏசிடு இருக்கீங்க ஆனா இவளோ மரியாதையை வேண்டாமே. டேய் அன்கர் கூதி எப்படி இருக்க அதுக்காக இவளோ கொச்சையா பேச வேண்டாம் கொஞ்சம் மரியாதையா பேசுங்க. பெரு ஆறுமுகம் தேனீ மாவட்டம் மாமியார் பெரு நிரஞ்சனா லவ் என்ற பொண்ணு பெரு ஸ்வீஎதா. உங்க லவ் ஸ்டோரி பத்தி சொல்லுங்க. சார் எட்டு வருசமா லவ் பண்றேன் நா கல்யாணம் பண்ண போறேன். எனக்கு ரொம்ப ஆசிரியமா இருக்கு லவ் பண்றவங்க கல்யாணம் பன்னுவாங்கனு. ',\n",
       " 'பஞ்சாயத்தை தேர்பத்துக்காக வந்து இருக்காங்க இந்த அருமை ஆனா நிகழ்வுல மோதலை பெத்தவங்க சைடு ல இருந்து பேசுவதற்காக ஒரு அருமை ஆனா பேச்சாளர். உங்க பெரு என்ன அரவிந்த் ஐயா எந்த ஊரு காடூர் ஐயா காடூர் ல இருந்து வந்துருக்கான் காட்டு வாசி பயன். பேசு டா undefined ஐயா நடுவர் அவர்களே சொல்லுடா சொல்லு ஐயா ஒரு ப்லொவ் ல பெபட விடுங்க ஐயா. நம எப்படி ஐயா இந்த உலகத்துக்கு வந்தோம் க்ஸ் கிரோமோசோம் ய கிரோமோசோம் அந்த கிருமிகளா வந்தோம். அப்டி இல்ல அப்டி இல்ல நா நீ இந்த வியாபாரி படத்துல டூப்ளே இருந்து சத்துணவு சாணி மாறி விழுந்தாயா. ',\n",
       " 'நம்ம அப்பா அம்மா தாக ஐயா. பத்து மாசம் சுமந்து பேதங்கள் அவங்கள பத்தி தா. நூத்துல ஒன்னு சூத்துல ரெண்டும் ர மாறி பேசிட்டாங்க. ஆனா இவனே கடைசி காலத்துல அவங்க அம்மா வுக்கு படுக்க பாய் கூட தரல ஆனா இப்போ அம்மாவை பத்தி பேசி காய் தட்டு வாங்கிட்டான். உங்க அம்மா மேல இருந்து பெருமையை பாத்துட்டு இருப்பாங்க. ஐயா நா இப்ப பேசப்போறது அப்பாவை பத்தி. எங்க அப்பா கூலி வேல செஞ்சு என்ன வலத்தாறு. என்ன கூலி வேல கார்பொரேட் கூலி ஐயா. மாசம் அறுபது அஞ்சாயிரம் தா சம்பளம். ',\n",
       " 'கார்பொரேட் கல்ச்சர் ரெண்டு டன் லடோம் அதே தாங்க ஐயா மாசம் அறுபது அஞ்சாயிரம் தா ஐயா சம்பளம் சம்பளம் கிடைச்சவுடனே ஷாட் போட ரிசார்ட் போய்டுவாங்க ஐயா ஆனா மூணு நாள் அந்த சம்பளத்தை முடுச்சிட்டு நாங்க இங்க பசியோட இருப்போம் அவரு எங்களுக்கு பண்ட்ரி ல இருந்து பிரட் பிசகுட் ஜாம் லாம் திருடிட்டுவந்து எங்களுக்கு பசி ஆத்துவாரு எங்க அப்பா அந்த தெய்வம் தா ஐயா எங்களுக்கு முக்கியம். இந்த பரதேசி பயண பெத்தவன் பேச ஒரு தனி தைரியம் வேணும். அப்படிப்பட்ட தைரியம் இவரு கிட்ட இருக்கு. ',\n",
       " 'என்ன பாதுகா ஒரு பொண்ணு வேணுமா இல்லையா இந்த நிகழ்ச்சியை குடும்பத்தோடு பாத்துட்டு இருப்பாங்க. அதுமட்டும் இல்ல நா கொஞ்சம் கோவக்காரன் சிரிச்சிட்டே தா இருப்பேன் ஆனா போடா கிறுக்கு புனை னு திட்டிடுவேன். அதுனால உங்க எதிர்பார்ப்பு என்ன அத மட்டும் சொல்லுங்க. மஹாலக்ஷ்மி மாறி ஒரு பொண்ணு வேணும் இப்பதா ரோஜாபூமாலை நிகழ்ச்சிக்கு எதை மாறி பேசி இருக்கீங்க. டிக் டொக் பண்ற மாறி மஹாலக்ஷ்மி சார் காஸ்ஸ்டவ்மர் பாத ரிஜெக்ட் ஏ பண்ணக்கூடாது. இது எந்த நிகழ்ச்சி னு தெரியுமா டா ரோஜாபூமாலை டா காஜுமாலை கிடையாது ட கொம்மாள. ',\n",
       " 'இவங்க பெத்தவங்க பெத்தவங்க னு சொல்றாங்களே நண்பன் இல்லாம இருக்க முடியுமா ஐயா தளபதி படத்துல ஸ்ரீ வித்யா பெத்தவங்கள பூட்ஸ் வண்டியில போட்டுடுவாயா. ஆனா நண்பன் கூட இருந்து பாத்துக்குவான் வந்தாயா நண்பன். பெரு என்ன டா ஏன் பெரு முழுவிந். எவன் பேரே விவகாரம் இருக்கு பேசுற கன்டென்ட் மட்டும் எனக்கு தெரியாது னு நினைக்காதீங்க. இந்த மாறி கன்டென்ட் ஐ பேசமுடியாமல் பேசும் இந்த தற்குறிப்பயலை வீனா போகவேண்டிய விந்தை உருவாக்கி அதை நண்பன் னு சொல்லிட்டு இருக்கான். ',\n",
       " 'இப்படி கண்ட கண்ட பாடலை அட்வைஸ் பண்ணாலும் கடைசி வரைக்கும் தோல் கொடுத்து நிபந்தய நண்பன் நா ஏன் நண்பன் த போலாம் னு இருந்தேன் போர்ப்பயே தெரியும் அவன் பேப்பர் நிறையா கன்டென்ட் கொடுத்து அனுப்புவான் ஐயா. நம ஒரு பொண்ண லவ் பண்ணா அப்பா திட்டுவாங்க அம்மா செருப்பால அடிப்பாங்க இவளோ ஏன் அந்த பொன்னே நம்மள விட்டுட்டு போகும் நா இருக்கேன் டா னு சொல்லி கடைசி வரைக்கும் நிக்கிறானே வந்தா ஐயா நண்பன். நண்பனை குனியவச்சு சுகம் பாக்கலாம் னு நினைக்குற நண்பர்களுக்கு என் மனமார்ந்த நன்றிகள். ',\n",
       " 'நாம் அனைவரும் வாழ்த்த பிறந்தவர்கள் வாழ்க்கையை வள அல்ல வாழ்த்த அதுமட்டும் அல்ல ஓழ்த்தவும் தான். ஆழ்த்தும் நம் பாரம்பரியத்தின் அங்கம் தானே அதை பத்தி பேச குழம்பி இருக்கிறது இந்த கூட்டம். ஓல் இந்த சொல் இந்த நூற்றாண்டில் சொல் என்று சொன்னால் உங்களால் நம்பமுடிகிறதா நம்புங்கள் எதை மேலோட்டமாக பார்த்தால் கலவி என்று பொருள் தரும் கொச்சை ஆனா சொல் என்று கேக்கலாம். கொஞ்சம் தர்க்கரீதியாக யோசித்து பாருங்களே பட்ட இலக்கியத்தை எளிய மக்களிடம் கொண்டு சேர்க்கும் ஒரு எழுத்தாளரின் கடைசி மூச்சில் ஒரு ஒற்றை வரி சொல் தான் இந்த ஓல். ',\n",
       " 'இது என்ன இன்று முளைத்த சொல்லலா இல்லை நூற்றாண்டு காலத்திற்கு முன்பு நண்பர்கள் எங்கு குழம்பி நின்றார்களோ அன்று முளைத்த சொல் நான்கு நண்பர்கள் குழம்பி நிற்கும் பொது முதல் ஓல் வரும் அதுதான் மங்கள ஓல். மங்கள ஓல் அது ஒரு தூண்டுகோல் அது நான்கு ஐந்து துணை ஓல்களை உருவாகும். இந்த ஐந்து ஆறு நண்பர்களில் முதலில் ஒருவன் சொல்வான் நான் அவளை நேத்தே போட்டுட்டேன் என்று. எதை பார்த்து காஜூ உண்டாகி இன்னோருத்தவன் சொல்வான் நான் அவளை ஒரு வருஷத்துக்கு முன்னாடியே போட்டுட்டேன் என்று. அதை பார்த்து இன்னோருத்தவன் சொல்வான் நான் அவளை ரெண்டு வருஷத்துக்கு முன்னாடியே போட்டுட்டேன் என்று. யாராவது இவர்கள் சொல்வது பொய் என்று கண்டறிந்துவிட்டால் ஒரு ஓழ்த்துப்பாடலை பாடி அவனை மகிலூட்டுவன். ஓலா ஓலா ஓலா ஓலா ஓலம் அ. நங்கள் எழுதிய பாடல் ஓல் எவ்வளவு நயமாக கையாளப்பட்டு இருக்கிறது என்று கேட்டு தெரிந்து கொள்ளுங்கள். ',\n",
       " 'ஓல் மிகச்சாதாரணமாக வருவது கிடையாது. சில சந்தர்ப்ப சூழ்நிலை கைகொடுக்கும். உதாரணமாக நா நண்பனிடம் காசுக்கேட்டால் ஓல் வரும். நா காசு கேப்பேன் அவன் என் காதை கேப்பான். நான் அதிகபச்சமாக ஐநூறு ரூபாய் கேட்டு இருப்பேன் அவன் அப்பாவுக்கு உடம்பு சரி இல்ல தங்கச்சிக்கு காது கேக்கல அம்மாவுக்கு கண்ணு தெரியல மாமாவுக்கு காலு வெளங்களை காசு இல்லை என்பான். திட்டத்தட்ட மொத குடும்பத்தையும் ஸ்ஓம்பிய மாத்திடுவான். இப்படி அவன் மரண ஓல் போகும்போதெல்லாம் என் காது பாவம். அப்போது எனக்கு நான் எழுதிய கவிதை ஒன்று நியாபகம் வரும் நண்பா ஓ நண்பா என் காது என்ன கூதியா ஓத்து விட்டு போகுறாயே நண்பா ஓ நண்பா காதுகள் பாவம் இல்லையா. ',\n",
       " 'இப்படி வகை துணை இல்லாமல் ஓல் போடும் நண்பனுக்கு துணை பெயர் மிக அதிகம் வாய் ஓல் வெங்கடராமன் பசை ஓல் பரந்தாமன் வெது ஓல் வெண்குடுப்பதி என்ன ஏராளம் எவை அனைத்தும் என்னை சாரும் அதனால் எனக்கு தலை கணம் சற்றும் இருந்தது இல்லை என்பதை இந்த மேடையில் தெளிவு படுத்தி என்னை வாழவைக்கும் ஓளுக்கு எனக்கு மேடை கொடுத்த ஓல் அரங்குக்கு எனது நன்றியை தெரிவித்து கொள்கிறேன். சப்பிசகிரிப் செய்து விடுங்கள் மணியை அமிக்கி விடுங்கள். ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lbl_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = Text_nab + Text_ab\n",
    "Lbl = np.concatenate((Lbl_nab, Lbl_ab), axis = 0)\n",
    "len(Text), len(Lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[Text, Lbl], index=[\"text\", \"label\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].progress_apply(preproc.remove_URL)\n",
    "df[\"text\"] = df[\"text\"].progress_apply(preproc.remove_html)\n",
    "df[\"text\"] = df[\"text\"].progress_apply(preproc.remove_emoji)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('h1.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df[\"text\"].map(lambda text: TextBlob(text).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Show the polarities repartition\n",
    "df['polarity'].hist()\n",
    "plt.xlabel(\"Polarity\")\n",
    "plt.ylabel(\"Number of text\")\n",
    "#plt.savefig(\"polarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Extract statistics of the text \n",
    "df['char_count'] = df[\"text\"].apply(len) # Number of characters in the string\n",
    "df['word_count'] = df[\"text\"].apply(lambda x: len(x.split())) # Number of words in the string \n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1) # Density of word (in char)\n",
    "df['punctuation_count'] = df[\"text\"].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "df['title_word_count'] = df[\"text\"].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()])) # Number of words containing upper letter in first place \n",
    "df['upper_case_word_count'] = df[\"text\"].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Show the number of characters per document\n",
    "plt.figure(figsize=(15,10))\n",
    "max_x = 10000 if df.char_count.max()>10000 else df.char_count.max()\n",
    "plt.hist(df.char_count.values, bins = range(0, max_x, 100))\n",
    "plt.title(f\"Number of Video Transcripts {df.shape[0]}\")\n",
    "plt.xlabel(\"Number of characters\")\n",
    "plt.ylabel(\"Number of documents\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(\"numb_char.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- histogram of the class frequency\n",
    "df[\"label\"].hist(xrot=45)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Class Frequency\")\n",
    "plt.savefig(\"distrib_classes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "df.loc[:,\"text_sw\"] = df.loc[:,\"text\"].progress_apply(lambda x : preproc.remove_stop_words(x, \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[\"text_sw\"].isnull().sum()>0:\n",
    "    print(\"Empty text\")\n",
    "    df[\"text_sw\"][df[\"text_sw\"].isnull()] = \"empty_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL = [\"Non_Abusive\", \"Abusive\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the transformers 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1 = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "embeddings_1 = model_1.encode(np.array(df[\"text\"]))\n",
    "\n",
    "train_x_1, valid_x_1, y_train_1, y_valid_1 = model_selection.train_test_split(np.array(embeddings_1), df[\"label\"], random_state=42, stratify=df[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = y_train_1.astype('int')\n",
    "y_valid_1 = y_valid_1.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_clf = SVC(kernel=\"poly\",C=100,gamma=1)\n",
    "svm_clf.fit(train_x_1,np.array(y_train_1))\n",
    "svm_pred = svm_clf.predict(valid_x_1)\n",
    "#print(classification_report(y_valid_1,svm_pred))\n",
    "\n",
    "metrics_ML(svm.SVC(kernel=\"poly\",C=100,gamma=1),  train_x_1, y_train_1, valid_x_1, y_valid_1, LBL, gb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "rf.fit(train_x_1,y_train_1)\n",
    "rf_pred = rf.predict(valid_x_1)\n",
    "# print(classification_report(y_valid_1,rf_pred))\n",
    "\n",
    "metrics_ML(ensemble.RandomForestClassifier(),  train_x_1, y_train_1, valid_x_1, y_valid_1, LBL, gb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logestic Regression\n",
    "lr = linear_model.LogisticRegression(max_iter=1000,  random_state=42)\n",
    "lr.fit(train_x_1,y_train_1)\n",
    "lr_pred = lr.predict(valid_x_1)\n",
    "# print(classification_report(y_valid_1,rf_pred))\n",
    "\n",
    "metrics_ML(linear_model.LogisticRegression(max_iter=1000,  random_state=42),  train_x_1, y_train_1, valid_x_1, y_valid_1, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "print(\"\\nStochastic Gradient Descent with early stopping for One-Hot encoding\\n\")\n",
    "print(\"Early Stopping : 10 iterations without change\")\n",
    "sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 )\n",
    "sgd.fit(train_x_1,y_train_1)\n",
    "sgd_pred = sgd.predict(valid_x_1)\n",
    "#print(classification_report(y_valid_1,sgd_pred))\n",
    "\n",
    "metrics_ML(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ),  train_x_1, y_train_1, valid_x_1, y_valid_1, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XgBOOST\n",
    "xbg = XGBClassifier(n_estimators=1000, subsample=0.8)\n",
    "eval_set = [(train_x_1, y_train_1), (valid_x_1, y_valid_1)]\n",
    "xbg.fit(train_x_1, y_train_1, eval_metric=[\"error\", \"logloss\"],  eval_set=eval_set,early_stopping_rounds=10, verbose=False) \n",
    "xgb_pred = xbg.predict(valid_x_1)\n",
    "#print(classification_report(y_valid_1,xgb_pred))\n",
    "\n",
    "metrics_ML(XGBClassifier(n_estimators=1000, subsample=0.8),  train_x_1, y_train_1, valid_x_1, y_valid_1, LBL, gb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the transformers 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "embeddings_2 = model_2.encode(np.array(df[\"text\"]))\n",
    "train_x_2, valid_x_2, y_train_2, y_valid_2 = model_selection.train_test_split(np.array(embeddings_2), df[\"label\"], random_state=42, stratify=df[\"label\"], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = y_train_2.astype('int')\n",
    "y_valid_2 = y_valid_2.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_clf = SVC(kernel=\"poly\",C=100,gamma=1)\n",
    "svm_clf.fit(train_x_2,y_train_2)\n",
    "svm_pred = svm_clf.predict(valid_x_2)\n",
    "print(classification_report(y_valid_2,svm_pred))\n",
    "\n",
    "metrics_ML(SVC(kernel=\"poly\",C=100,gamma=1),  train_x_2, y_train_2, valid_x_2, y_valid_2, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "rf.fit(train_x_2,y_train_2)\n",
    "rf_pred = rf.predict(valid_x_2)\n",
    "#print(classification_report(y_valid_2,rf_pred))\n",
    "\n",
    "metrics_ML(ensemble.RandomForestClassifier(),  train_x_2, y_train_2, valid_x_2, y_valid_2, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logestic Regression\n",
    "lr = linear_model.LogisticRegression(max_iter=1000,  random_state=42)\n",
    "lr.fit(train_x_2,y_train_2)\n",
    "lr_pred = lr.predict(valid_x_2)\n",
    "# print(classification_report(y_valid_2,rf_pred))\n",
    "\n",
    "metrics_ML(linear_model.LogisticRegression(max_iter=1000,  random_state=42),  train_x_2, y_train_2, valid_x_2, y_valid_2, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "print(\"\\nStochastic Gradient Descent with early stopping for One-Hot encoding\\n\")\n",
    "print(\"Early Stopping : 10 iterations without change\")\n",
    "sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 )\n",
    "sgd.fit(train_x_2,y_train_2)\n",
    "sgd_pred = sgd.predict(valid_x_2)\n",
    "#print(classification_report(y_valid_2,sgd_pred))\n",
    "\n",
    "metrics_ML(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ),  train_x_2, y_train_2, valid_x_2, y_valid_2, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XgBOOST\n",
    "xbg = XGBClassifier(n_estimators=1000, subsample=0.8)\n",
    "eval_set = [(train_x_2, y_train_2), (valid_x_2, y_valid_2)]\n",
    "xbg.fit(train_x_2, y_train_2, eval_metric=[\"error\", \"logloss\"],  eval_set=eval_set,early_stopping_rounds=10, verbose=False) \n",
    "xgb_pred = xbg.predict(valid_x_2)\n",
    "#print(classification_report(y_valid_2,xgb_pred))\n",
    "\n",
    "metrics_ML(XGBClassifier(n_estimators=1000, subsample=0.8),  train_x_2, y_train_2, valid_x_2, y_valid_2, LBL, gb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lodading transformer 3 part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_vals(df_col):\n",
    "    A=[]\n",
    "    for i in df_col:\n",
    "        t = [0]*max(np.unique(df_col)+1)\n",
    "        t[i]=1\n",
    "        A.append(t)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_31 = SentenceTransformer('monsoon-nlp/tamillion')\n",
    "#embeddings_31 = model_31.encode(np.array(df[\"text\"]))\n",
    "train_x_31, valid_x_31, y_train_31, y_valid_31 = model_selection.train_test_split(df, df[\"label\"], random_state=42, stratify=df[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_frame = pd.DataFrame(train_x_31) \n",
    "eval_frame['class'] = categorical_vals(le.fit_transform(df[\"label\"]))\n",
    "#model_31.fit(train_x_31,y_train_31)\n",
    "print(eval_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "# set use_cuda=False on CPU-only platforms\n",
    "model = MultiLabelClassificationModel('bert', 'monsoon-nlp/tamillion', num_labels=2, use_cuda=False,\n",
    "                            args={\n",
    "                'reprocess_input_data': True,\n",
    "                'use_cached_eval_features': False,\n",
    "                'overwrite_output_dir': True,\n",
    "                'num_train_epochs': 1,\n",
    "     \n",
    "})\n",
    "model.train_model(df)#,eval_data=eval_frame)\n",
    "print(\"trained model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading transformer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = SentenceTransformer('monsoon-nlp/tamillion')\n",
    "embeddings_3 = model_3.encode(np.array(df[\"text\"]))\n",
    "train_x_3, valid_x_3, y_train_3, y_valid_3 = model_selection.train_test_split(np.array(embeddings_3), df[\"label\"], random_state=42, stratify=df[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3 = y_train_3.astype('int')\n",
    "y_valid_3 = y_valid_3.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_clf = SVC(kernel=\"poly\",C=100,gamma=1)\n",
    "svm_clf.fit(train_x_3,y_train_3)\n",
    "svm_pred = svm_clf.predict(valid_x_3)\n",
    "#print(classification_report(y_valid_3,svm_pred))\n",
    "\n",
    "metrics_ML(SVC(kernel=\"poly\",C=100,gamma=1),  train_x_3, y_train_3, valid_x_3, y_valid_3, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "rf.fit(train_x_3,y_train_3)\n",
    "rf_pred = rf.predict(valid_x_3)\n",
    "#print(classification_report(y_valid_3,rf_pred))\n",
    "\n",
    "metrics_ML(ensemble.RandomForestClassifier(),  train_x_3, y_train_3, valid_x_3, y_valid_3, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logestic Regression\n",
    "lr = linear_model.LogisticRegression(max_iter=1000,  random_state=42)\n",
    "lr.fit(train_x_3,y_train_3)\n",
    "lr_pred = lr.predict(valid_x_3)\n",
    "# print(classification_report(y_valid_3,rf_pred))\n",
    "\n",
    "metrics_ML(linear_model.LogisticRegression(max_iter=1000,  random_state=42),  train_x_3, y_train_3, valid_x_3, y_valid_3, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "print(\"\\nStochastic Gradient Descent with early stopping for One-Hot encoding\\n\")\n",
    "print(\"Early Stopping : 10 iterations without change\")\n",
    "sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 )\n",
    "sgd.fit(train_x_3,y_train_3)\n",
    "sgd_pred = sgd.predict(valid_x_3)\n",
    "# print(classification_report(y_valid_3,sgd_pred))\n",
    "\n",
    "metrics_ML(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ),  train_x_3, y_train_3, valid_x_3, y_valid_3, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XgBOOST\n",
    "xbg = XGBClassifier(n_estimators=1000, subsample=0.8)\n",
    "eval_set = [(train_x_3, y_train_3), (valid_x_3, y_valid_3)]\n",
    "xbg.fit(train_x_3, y_train_3, eval_metric=[\"error\", \"logloss\"],  eval_set=eval_set,early_stopping_rounds=10, verbose=False) \n",
    "xgb_pred = xbg.predict(valid_x_3)\n",
    "#print(classification_report(y_valid_3,xgb_pred))\n",
    "\n",
    "metrics_ML(XGBClassifier(n_estimators=1000, subsample=0.8),  train_x_3, y_train_3, valid_x_3, y_valid_3, LBL, gb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading transformer 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = SentenceTransformer('google/muril-base-cased')\n",
    "embeddings_5 = model_5.encode(np.array(df[\"text\"]))\n",
    "train_x_5, valid_x_5, y_train_5, y_valid_5 = model_selection.train_test_split(np.array(embeddings_5), df[\"label\"], random_state=42, stratify=df[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = y_train_5.astype('int')\n",
    "y_valid_5 = y_valid_5.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm_clf = SVC(kernel=\"poly\",C=100,gamma=1)\n",
    "svm_clf.fit(train_x_5,y_train_5)\n",
    "svm_pred = svm_clf.predict(valid_x_5)\n",
    "#print(classification_report(y_valid_5,svm_pred))\n",
    "\n",
    "metrics_ML(SVC(kernel=\"poly\",C=100,gamma=1),  train_x_5, y_train_5, valid_x_5, y_valid_5, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "rf.fit(train_x_5,y_train_5)\n",
    "rf_pred = rf.predict(valid_x_5)\n",
    "#print(classification_report(y_valid_5,rf_pred))\n",
    "\n",
    "metrics_ML(ensemble.RandomForestClassifier(),  train_x_5, y_train_5, valid_x_5, y_valid_5, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logestic Regression\n",
    "lr = linear_model.LogisticRegression(max_iter=1000,  random_state=42)\n",
    "lr.fit(train_x_5,y_train_5)\n",
    "lr_pred = lr.predict(valid_x_5)\n",
    "# print(classification_report(y_valid_5,rf_pred))\n",
    "\n",
    "metrics_ML(linear_model.LogisticRegression(max_iter=1000,  random_state=42),  train_x_5, y_train_5, valid_x_5, y_valid_5, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "print(\"\\nStochastic Gradient Descent with early stopping for One-Hot encoding\\n\")\n",
    "print(\"Early Stopping : 10 iterations without change\")\n",
    "sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 )\n",
    "sgd.fit(train_x_5,y_train_5)\n",
    "sgd_pred = sgd.predict(valid_x_5)\n",
    "# print(classification_report(y_valid_5,sgd_pred))\n",
    "\n",
    "metrics_ML(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ),  train_x_5, y_train_5, valid_x_5, y_valid_5, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XgBOOST\n",
    "xbg = XGBClassifier(n_estimators=1000, subsample=0.8)\n",
    "eval_set = [(train_x_5, y_train_5), (valid_x_5, y_valid_5)]\n",
    "xbg.fit(train_x_5, y_train_5, eval_metric=[\"error\", \"logloss\"],  eval_set=eval_set,early_stopping_rounds=10, verbose=False) \n",
    "xgb_pred = xbg.predict(valid_x_5)\n",
    "#print(classification_report(y_valid_5,xgb_pred))\n",
    "\n",
    "metrics_ML(XGBClassifier(n_estimators=1000, subsample=0.8),  train_x_5, y_train_5, valid_x_5, y_valid_5, LBL, gb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14958d3aee5f1cad06795f787e54b96185c25fb40dfec723a5be941f3a531b8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
